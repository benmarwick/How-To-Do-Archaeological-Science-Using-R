[
["index.html", "How To Do Archaeological Science Using R Chapter 1 Introduction", " How To Do Archaeological Science Using R Ben Marwick (editor) 2017-03-30 Chapter 1 Introduction Ben Marwick This website is a early draft of an edited volume of contributions to the ‘How To Do Archaeological Science Using R’ forum of the 2017 Society of American Archaeology annual meeting in Vancouver, BC. The forum was organised by Ben Marwick, who is the editor of this collection. The chapters here are early drafts and are still in preparation. If you see a typo, you can make a correction by editing the source files at https://github.com/benmarwick/How-to-do-archaeological-science-using-R. Archaeological science is becoming increasingly complex, and progress in this area is slowed by critical limitation of journal articles lacking the space to communicate new methods in enough detail to allow others to reproduce and reuse new research. One solution to this is to use a programming language such as R to analyse archaeological data, with authors sharing their R code with their publications to communicate our methods. This practice is becoming widespread in many other disciplines, but few archaeologists currently know how to use R or have an opportunity to learn during their training. In this forum we tackle this problem by discussing ubiquitous research methods of immediate relevance to most archaeologists, by using interactive, live-coded demonstrations of R code by archaeologists who program with R. Topics include getting data into R, working with C14 dates, spatial analysis and map-making, conducting simulations, and exploratory data visualizations. Each chapter represents one of the R code demonstrations presented during the forum. All of the code is runnable, so that the code shown in the chapter generates the results seen in the chapter. In these chapters we raise the code to a first-class reserach product so that other archaeologists can peek ‘under the hood’ to see how R can be used for archaeological science. All code in this volume is licenced CC-BY, and is freely available for reuse if the author is appropriately credited. Our goal is to speed the progress of archaeological science by sharing code freely for others to adapt and extend. "],
["basic-spatial-analysis-in-r-point-pattern-analysis.html", "Chapter 2 Basic spatial analysis in R: Point pattern analysis", " Chapter 2 Basic spatial analysis in R: Point pattern analysis Phil Riris (Institute of Archaeology, UCL) 2.0.1 Introduction This demo is prepared for the SAA 2017 forum at Vancouver “How to do archaeological science using R”. This demo has three goals: Demonstrate how to load two types of spatial data (vectors and rasters) and plot them Demonstrate data coercion into useful formats for spatial analysis Explore spatial dependency based on a) between-point interaction, b) external covariates For more examples of spatial stats applied to archaeological questions, see: Riris, P. 2017. “Towards an artefact’s-eye view: Non-site analysis of discard patterns and lithic technology in Neotropical settings with a case from Misiones province, Argentina” Journal of Archaeological Science: Reports 11 DOI: http://dx.doi.org/10.1016/j.jasrep.2017.01.002 2.0.1.1 Loading and plotting spatial data Suppose we have surveyed a prehistoric settlement landscape in full, and we are reasonably certain all major sites have been located by our awesome field team. We want to understand the empirical pattern in front of us, specifically how smaller sites not only outnumber, but also seem to group around larger and more important sites. How strong is this pattern? Does it hold equally across the whole study area? Can we explain the distribution of a type of site in terms of that of another? Here are the R packages that we will need to answer these questions: require(spatstat) # Load packages require(sp) require(maptools) require(rgdal) require(raster) With time and money limited and excavation difficult, we can’t return to the field, so we have to resort to the data we have: in this case a basic distribution map! We will import our field data into R, briefly investigate it to ensure that it looks right: main.sites &lt;- readShapeSpatial(&quot;main.sites.shp&quot;) # Load our sites from shapefile secondary.sites &lt;- readShapeSpatial(&quot;secondary.sites.shp&quot;) elev &lt;- raster(&quot;elevation.tif&quot;) # Load elevation map str(main.sites) # Investigate and check that the files have loaded correctly. ## Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots ## ..@ data :&#39;data.frame&#39;: 21 obs. of 1 variable: ## .. ..$ marks: Factor w/ 1 level &quot;Primary&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## .. ..- attr(*, &quot;data_types&quot;)= chr &quot;C&quot; ## ..@ coords.nrs : num(0) ## ..@ coords : num [1:21, 1:2] 0.0442 0.4661 0.8143 0.3285 0.7165 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:21] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ... ## .. .. ..$ : chr [1:2] &quot;coords.x1&quot; &quot;coords.x2&quot; ## ..@ bbox : num [1:2, 1:2] 0.0314 0.0483 0.9766 0.9296 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2] &quot;coords.x1&quot; &quot;coords.x2&quot; ## .. .. ..$ : chr [1:2] &quot;min&quot; &quot;max&quot; ## ..@ proj4string:Formal class &#39;CRS&#39; [package &quot;sp&quot;] with 1 slot ## .. .. ..@ projargs: chr NA str(secondary.sites) ## Formal class &#39;SpatialPointsDataFrame&#39; [package &quot;sp&quot;] with 5 slots ## ..@ data :&#39;data.frame&#39;: 105 obs. of 1 variable: ## .. ..$ marks: Factor w/ 1 level &quot;Secondary&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## .. ..- attr(*, &quot;data_types&quot;)= chr &quot;C&quot; ## ..@ coords.nrs : num(0) ## ..@ coords : num [1:105, 1:2] 0.0432 0.1002 0.1007 0.0414 0.4581 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:105] &quot;0&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; ... ## .. .. ..$ : chr [1:2] &quot;coords.x1&quot; &quot;coords.x2&quot; ## ..@ bbox : num [1:2, 1:2] 0.00799 0.01278 0.9964 0.98552 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2] &quot;coords.x1&quot; &quot;coords.x2&quot; ## .. .. ..$ : chr [1:2] &quot;min&quot; &quot;max&quot; ## ..@ proj4string:Formal class &#39;CRS&#39; [package &quot;sp&quot;] with 1 slot ## .. .. ..@ projargs: chr NA With the data imported and checked, we can make a simple exploratory plot: # png(filename = paste0(getwd(), &quot;/&quot;, &quot;Sites.png&quot;), # width = 600, # height = 600, # units = &quot;px&quot;) plot(elev, main = &quot;Study Area&quot;, axes = FALSE) # Inspect data plot(main.sites, col = &quot;red&quot;, pch = 16, add = TRUE) plot(secondary.sites, col = &quot;blue&quot;, pch = 17, add = TRUE) legend(&quot;bottomleft&quot;, title = &quot;Site types&quot;, c(&quot;Primary&quot;, &quot;Secondary&quot;), pch = c(16, 17), col = c(&quot;red&quot;, &quot;blue&quot;)) # dev.off() 2.0.1.2 Querying spatial data For visualisation of raw patterns and classic exploratory tools we must coerce our data into a ‘ppp’ (planar point pattern) object for spatial statistics using the spatstat library, and then compute the intensity of the data (or ‘stuff per unit area’): main.pp &lt;- as.ppp(main.sites) # Coerce into &#39;ppp&#39; object for spatial statistics secondary.pp &lt;- as.ppp(secondary.sites) sites.pp &lt;- superimpose(main.pp, secondary.pp) intensity(sites.pp) # Measure intensity of different patterns ## Primary Secondary ## 21.84156 109.20781 We can also compute the intesity for all sites together: allsites &lt;- unmark(sites.pp) # removes &quot;marks&quot; (i.e. descriptors) from point pattern intensity(allsites) # Intensity (points/unit of area) = 131.0494 ## [1] 131.0494 This is not very informative on its own - an abstract number. Let’s see if we can get more insights from additional visualisations and stats: d.sites &lt;- density.ppp(sites.pp) # Overall density of all points in our data plot(d.sites, main = &quot;Density of all sites&quot;) # Plot interpolated values plot(sites.pp, add = T) For numeric marks (e.g. number of structures per settlement) we would use smooth.ppp() function. But is this informative on its own? We have two different types of site. par(mfrow=c(1,2)) # Set number of plotting rows to 1 and number of columns to 2 plot(density(main.pp), main = &quot;Density of primary settlements&quot;) plot(density(secondary.pp), main = &quot;Density of satellite settlements&quot;) Visualisations alone can be misleading. The patterns are clearly related, but how do we formally characterise this? First, it’s important to make note of any significant spatial relationships: clarkevans.test(secondary.pp, correction = c(&quot;Donnelly&quot;), nsim = 999) ## ## Clark-Evans test ## Donnelly correction ## Monte Carlo test based on 999 simulations of CSR with fixed n ## ## data: secondary.pp ## R = 0.65946, p-value = 0.002 ## alternative hypothesis: two-sided R &lt; 1 among secondary settlements clarkevans.test(main.pp, correction = c(&quot;Donnelly&quot;), nsim = 999) ## ## Clark-Evans test ## Donnelly correction ## Monte Carlo test based on 999 simulations of CSR with fixed n ## ## data: main.pp ## R = 1.053, p-value = 0.718 ## alternative hypothesis: two-sided R &gt; 1 among primary sites, but not significant Tentative conclusion: there are two different processes creating the observed distribution of sites 2.0.1.3 Exploring spatial relationships More formally, we can state our null hypothesis as: Primary and secondary sites are realisations of two independent point processes (different cultural strategies?) A point process is simply a process (random or otherwise) that generates an empirical pattern. We know in this case that they are almost certainly related, but it is always worthwhile to go through the motions in case of anything unexpected cropping up. A good place to start is bivariate spatial statistics such as the bivariate pair-correlation function. It tests for clustering or dispersion between two point patterns, i.e. the influence of one on the other. The “classic” pair-correlation function and its ancestor Ripley’s K are both univariate, but still worth using. However, here, we focus on interaction and dependency between two site-types. plot(pcfcross( sites.pp, divisor = &quot;d&quot;, correction = &quot;Ripley&quot;), main = &quot;Bivariate pair-correlation function&quot;) Empirical pattern obviously differs greatly from function under spatial randomness: g(pois), but how greatly? We can use the same approach and simulate envelopes of significance to compare agains the empirical pattern. bivar &lt;- envelope( fun = pcfcross, sites.pp, divisor = &quot;d&quot;, nsim = 99, correction = c(&quot;Ripley&quot;) ) ## Generating 99 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, ## 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, ## 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99. ## ## Done. plot(bivar, main = &quot;Pair-correlation function, with simulation envelope&quot;) This function shows us the following: very strong association at short ranges (up to 10 km), but around 12 km and 16-17 km there are actually fewer secondary sites than expected. A univariate function shows this dispersion to be present in the whole dataset. In fact, this matches the findings of the Clark-Evans NN test. What may have caused secondary sites to be clustered to primary sites, but primary sites to be dispersed? Now we know: a) the distribution of sites varies in space, b) the presence of one type of site is tied to the presence of another. We can now take the step from exploratory spatial analysis to confirmatory. 2.0.1.4 Point process modelling Define a spatstat window object for primary sites, and compute a kernel density estimate of primary settlement sites, to act as covariate, then fit point process model with the covariate: pwin &lt;- as.owin(c(-0.1, 1.1, -0.1, 1.1)) # main.pp$window &lt;- pwin # Assign window pden &lt;- density(main.pp) # Kernel density estimate of primary settlement sites, to act as covariate fit.sec &lt;- ppm(unmark(secondary.pp), ~ Cov1, covariates = list(Cov1 = pden)) # Fit point process model with the covariate This appears to be somewhat acceptable model; locations of secondary sites seem to depend on those of primary sites. But how good is this fit? # png(filename = &quot;density.png&quot;, width = 600, height = 600, units = &quot;px&quot;) plot(predict(fit.sec), main=&quot;Predicted distribution of secondary settlements&quot;, col = rainbow(255)) points(secondary.pp) # Appears to be somewhat acceptable model; # locations of secondary sites seem to depend # on those of primary sites! But how good is this fit? #dev.off() The Residual K function indicates that the locations of satellite settlements, although spatially autocorrelated with the principal settlements, are poorly explained by using only this covariate. Although a good fit is found between ~ 14 and ~18 km (the purple lines), at scales above and below this narrow band there is both significantly more dispersion and clustering, respectively. What may explain this statistical pattern? fit.res &lt;- envelope( fun = Kres, secondary.pp, model = fit.sec, nsim = 19, correction = c(&quot;Ripley&quot;)) # Examine ## Generating 19 simulations of CSR ... ## 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19. ## ## Done. par(mfrow = c(1,2)) # Plot with two columns # png(filename = &quot;statplot.png&quot;, width = 600, height = 600, units = &quot;px&quot;) plot(fit.res, main=&quot;Residual K&quot;) abline(v = 0.14, col = &quot;purple&quot;) abline(v = 0.18, col = &quot;purple&quot;) plot(predict(fit.sec), main = &quot;Predicted distribution&quot;, col = rainbow(255)) points(secondary.pp) # dev.off() plot(fit.res, main = &quot;Residual K&quot;) abline(v = 0.14, col = &quot;purple&quot;) abline(v = 0.18, col = &quot;purple&quot;) plot(predict(fit.sec), main = &quot;Predicted distribution&quot;, col = rainbow(255)) points(secondary.pp) 2.0.2 Summary This exercise invites us to: Think critically about the relationships we think we see in our data, Formalise our hypotheses about the archaeological record and test them, Pursue further data collection and analysis when we are proven wrong. sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] raster_2.5-8 rgdal_1.2-5 maptools_0.9-2 sp_1.2-4 ## [5] spatstat_1.50-0 rpart_4.1-10 nlme_3.1-131 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.10 knitr_1.15.17 tensor_1.5 ## [4] magrittr_1.5 spatstat.utils_1.4-1 lattice_0.20-34 ## [7] stringr_1.2.0 tools_3.3.3 grid_3.3.3 ## [10] mgcv_1.8-17 deldir_0.1-12 htmltools_0.3.5 ## [13] yaml_2.1.14 abind_1.4-5 goftest_1.0-4 ## [16] rprojroot_1.2 digest_0.6.12 bookdown_0.3.16 ## [19] Matrix_1.2-8 fftwtools_0.9-8 evaluate_0.10 ## [22] rmarkdown_1.4 polyclip_1.6-1 stringi_1.1.3 ## [25] backports_1.0.5 foreign_0.8-67 "],
["using-linear-models-to-explore-childrens-institutions-in-urban-areas.html", "Chapter 3 Using linear models to explore children’s institutions in urban areas", " Chapter 3 Using linear models to explore children’s institutions in urban areas Paulina Przystupa 3.0.1 Introduction Hi, my name is Paulina and I was interested in looking at the placement of children’s institutions through time in relation to urban areas. Historical documents suggest that in the United States starting in the late 1800s and into the 1900s more and more Americans believed that rural and natural environments were good for raising children. These ideas started appearing in child-rearing literature initially aimed at middle-class parents. However, I wondered if these same beliefs applied to those who ran children’s institutions. Children’s institutions, such as orphanages and Native American Boarding schools, also rose to prominence at this time but were situated between the middle-class sensibilities of those forming such institutions and the practical service such places provided. To see if such institutions conformed to this idea I collected a sample of 62 children’s institutions, half Native American boarding schools, and half Orphanages to see if through time their relationship to urban areas changed. Specifically, I was interested in examining it as a linear trend, one where through time such institutions are built farther from urban areas. 3.0.1.1 Import the data into R After collecting my data, which consisted of a list of institutions, and finding out the distances, I used as-the-crow-flies distance to their city hall as my measure, I loaded the comma separated file or csv into R. You can save CSV files from excel and a lot of other workbook or table formats. You can also just load tables directly into R. child = read.csv(&quot;Combined_Orph_NA_data.csv&quot;) 3.0.1.2 Exploratory data analysis I wanted to just look at the data together and see what the relationship to time was so I did a basic scatter plot plot(Distance_KM ~ Year, data = child) It doesn’t look like a particularly strong trend but I can test this by fitting a linear regression model to see what is going on with my data using: child.lm = lm(Distance_KM ~ Year, data = child) summary(child.lm) ## ## Call: ## lm(formula = Distance_KM ~ Year, data = child) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.39 -39.15 -32.77 -10.56 580.95 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -556.8124 1021.2532 -0.545 0.588 ## Year 0.3148 0.5382 0.585 0.561 ## ## Residual standard error: 98.38 on 60 degrees of freedom ## Multiple R-squared: 0.005668, Adjusted R-squared: -0.0109 ## F-statistic: 0.342 on 1 and 60 DF, p-value: 0.5609 This creates a linear regression of the dependent variable, distance in kilometers, to the independent variable time, as stated in years, and provides me with the coefficients for the intercept, which is listed under the Estimate and (intercept), and slope, which is the estimate under the year, as well as the R-squared values. R squared is a metric that allows us to understand how well the linear regression model explains our data. The call for linear model lm() includes two types of R squared, multiple and adjusted. It looks like for this case the multiple R-squared, or just R-squared, is .005 which means that only about half a percent of the data is predicted by our model. The adjusted R-squared includes the number of points used to create the model. Essentially it examines how meaningful the R is based on the n of your sample. This is so you can compare the strength of different models with different samples used to calculate them. It can go up or down depending on the sample. In this case, the adjusted-R squared does not improve when we adjust for the size of the sample. Lastly, the p-value, which is the liklihood that this is random, is nowhere near significant. So the combination of the p-value and my rather unhelpful R-squared values suggests there’s limited explanatory value in a linear regression for this sample combined sample. 3.0.1.3 Residuals While I’m here I’ll also plot the residuals for the data set overall to see if there are any trends with those, which may alter my choice in data set or the methods I should apply to it. child.res = resid(child.lm) plot(child$Year, child.res) abline(h = 0, col = &quot;red&quot;) summary(lm(child.res~child$Year)) ## ## Call: ## lm(formula = child.res ~ child$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -54.39 -39.15 -32.77 -10.56 580.95 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.299e-13 1.021e+03 0 1 ## child$Year 2.332e-16 5.382e-01 0 1 ## ## Residual standard error: 98.38 on 60 degrees of freedom ## Multiple R-squared: 2.221e-32, Adjusted R-squared: -0.01667 ## F-statistic: 1.332e-30 on 1 and 60 DF, p-value: 1 Looking at the plot it doesn’t appear that there is any trend in the residuals and further more it has a p-value of 1 suggesting it is completely random. Which is what we want. If there were at trend in the residuals we might be looking at much more complicated data set and other statistical analysis of the data. 3.0.1.4 Subsetting the data However, as I noted earlier that I included two different types of children’s institution, so maybe each have different trends. To examine them separately I can use subset NAB = subset(child, Instit_Type == &quot;NAB&quot;) EAO = subset(child, Instit_Type == &quot;O&quot;) So now I can look at them separately, plotting distance per year for each institution plot(NAB$Distance_KM ~ NAB$Year, main = &quot;Distance from urban area vs. year for Native American boarding schools&quot;) abline(lm(NAB$Distance_KM~NAB$Year), col = &quot;red&quot;) plot(EAO$Distance_KM~EAO$Year, main = &quot;Distance from urban area vs. year for Orphanages&quot;) abline(lm(EAO$Distance_KM~EAO$Year), col = &quot;red&quot;) This view looks like there may be some increasing trends but there are some significant outliers. Looking at the R-squared for those trend lines summary(lm(NAB$Distance_KM ~ NAB$Year)) ## ## Call: ## lm(formula = NAB$Distance_KM ~ NAB$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -125.35 -61.93 -50.54 7.54 521.78 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1922.736 2248.163 -0.855 0.399 ## NAB$Year 1.052 1.186 0.887 0.382 ## ## Residual standard error: 130.8 on 29 degrees of freedom ## Multiple R-squared: 0.02641, Adjusted R-squared: -0.007167 ## F-statistic: 0.7865 on 1 and 29 DF, p-value: 0.3825 summary(lm(EAO$Distance_KM ~ EAO$Year)) ## ## Call: ## lm(formula = EAO$Distance_KM ~ EAO$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.499 -8.345 -5.816 -3.257 112.002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -163.22363 310.73304 -0.525 0.603 ## EAO$Year 0.09112 0.16357 0.557 0.582 ## ## Residual standard error: 23.67 on 29 degrees of freedom ## Multiple R-squared: 0.01059, Adjusted R-squared: -0.02353 ## F-statistic: 0.3103 on 1 and 29 DF, p-value: 0.5818 3.0.1.5 Removing outliers The r-squared values are still really low and my p-values are not significant. So it might be useful to remove the outliers. However, it doesn’t look like they have outliers at the same distance so for the orphanages I’m only going to look at ones that were less than 50 km, which removes two of my locations, while for Native American boarding schools I looked at ones that were less than 100 km. NAB_no = subset(NAB, NAB$Distance_KM &lt; 100) EAO_no = subset(EAO, EAO$Distance_KM &lt; 50) Then I re-plot them plot(NAB_no$Distance_KM~NAB_no$Year, main = &quot;Distance from urban area vs. year for Native American boarding schools, No Outliers&quot;) abline(lm(NAB_no$Distance_KM ~ NAB_no$Year), col = &quot;red&quot;) plot(EAO_no$Distance_KM~EAO_no$Year, main = &quot;Distance from urban area vs. year for Orphanages, No outliers&quot;) abline(lm(EAO_no$Distance_KM ~ EAO_no$Year), col = &quot;red&quot;) So some trends, perhaps ones I’m not very happy with considering one of them is negative , but lets look at a summary of the regression lines: summary(lm(NAB_no$Distance_KM ~ NAB_no$Year)) ## ## Call: ## lm(formula = NAB_no$Distance_KM ~ NAB_no$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -17.729 -12.717 -8.454 13.324 41.998 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 274.6597 353.0894 0.778 0.445 ## NAB_no$Year -0.1360 0.1863 -0.730 0.473 ## ## Residual standard error: 18.23 on 22 degrees of freedom ## Multiple R-squared: 0.02366, Adjusted R-squared: -0.02072 ## F-statistic: 0.5331 on 1 and 22 DF, p-value: 0.473 summary(lm(EAO_no$Distance_KM ~ EAO_no$Year)) ## ## Call: ## lm(formula = EAO_no$Distance_KM ~ EAO_no$Year) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.018 -2.991 -1.268 1.083 12.644 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -55.77391 55.17205 -1.011 0.321 ## EAO_no$Year 0.03165 0.02904 1.090 0.285 ## ## Residual standard error: 4.013 on 27 degrees of freedom ## Multiple R-squared: 0.04215, Adjusted R-squared: 0.006675 ## F-statistic: 1.188 on 1 and 27 DF, p-value: 0.2853 3.0.2 Summary Unfortunately neither of those was significant either. So it looks like linear regression, regardless of whether the institutions are lumped together, institutions are separated by type, and whether or not they include outliers does not support the hypothesis that children’s homes were built farther from urban areas through time. They all have very low R-squared values and insignificant p-values. However, there may be some interesting differences examining the trends before and after 1900, which I explored in other presentations. So while linear regression might not fit this data other methods may help us to understand what sort of trend we may be seeing. sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets base ## ## loaded via a namespace (and not attached): ## [1] backports_1.0.5 bookdown_0.3.16 magrittr_1.5 rprojroot_1.2 ## [5] tools_3.3.3 htmltools_0.3.5 yaml_2.1.14 Rcpp_0.12.10 ## [9] stringi_1.1.3 rmarkdown_1.4 knitr_1.15.17 methods_3.3.3 ## [13] stringr_1.2.0 digest_0.6.12 evaluate_0.10 "],
["agent-based-modelling-prehistoric-landscape-use-with-r-and-netlogo.html", "Chapter 4 Agent-based modelling prehistoric landscape use with R and NetLogo", " Chapter 4 Agent-based modelling prehistoric landscape use with R and NetLogo Ben Davies Built using R version 3.3.3 and NetLogo version 6.0. 4.0.1 Introduction Where I work in western New South Wales, Australia, heat retainer hearths are a common feature of archaeological landscapes. The hearths are manifest as concentrations of fire-altered stone on the surface. Sometimes, these concentrations protect dense caps of burned sediment and charcoal, the latter of which can be used to date the hearths. The temporal distribution of dates obtained from hearths at Rutherfords Creek show two patterns: a superlinear increase in the frequency of dates through time, and episodic gaps in the chronometric data. The increasing frequency of dates is typically explained in terms of either population growth or taphonomic loss, while the gaps are variously explained as being the result of periods of temporary abandonment, or cyclical expanasions and contractions of foraging ranges. The visibility of these hearths, and their retention of charcoal, is largely a product of a fluvial geomorphic environment, where episodic flood move sediment around the landscape. This tutorial draws on a study aimed at understanding how the frequency of erosion and deposition affects the distribution of datable features in a surface context. 4.0.2 Agent-based modeling with NetLogo and R This tutorial demonstrates the use of the RNetLogo package to facilitate the analysis of NetLogo agent-based models. Agent-based models are computer simulations in which individual system components (often in the form of autonomous computational “agents”) interact with each other and/or their environment according to a given set of rules. These micro-level interactions can generate macro-level regularities over time, allowing the modeller to observe the emergence of these larger patterns or entities as outcomes of smaller-scale activities. NetLogo is a modeling environment used to build agent-basd models. NetLogo can interface directly with R in two ways: either through an extension that comes bundled with NetLogo , or through the RNetLogo package in R. Both of these were developed by Jan C. Thiele. This tutorial uses a NetLogo model called HMODEL, aimed at understanding how the frequency of erosion and deposition affects the distribution of datable features in a surface context. Before getting started, you’ll need to make sure that you have NetLogo, R, and the RNetLogo package installed. 4.0.3 Starting NetLogo from R First, add the RNetLogo package to your current R session: Next, we need to tell R where to find NetLogo is stored, specifically where the NetLogo.jar file is stored. On a Windows machine, this is probably somewhere like C:\\Program Files\\NetLogo 6.0\\app (this can be edited below to locate this file on your machine). Once the NetLogo.jar file has been located, the code will identify the directory where it is located, then start NetLogo from R. To open a model, we need to tell R where the NetLogo file is located using the NLLoadModel function. The working directory, however, now been changed from the folder where this document is located to the folder where the NetLogo This code will open the hmodel.nlogo file by referring back to the home folder. 4.0.4 Running a NetLogo model from R The RNetLogo extension calls out to NetLogo’s command line using the NLCommand and NLDoCommand functions. These will either perform one command, or perform them a preset number of times. So we can use NLCommand to run the setup routine and set parameters (in this case, surface stability), and we can use NLDoCommand to run the simulation for 2000 time steps (on the 2001st step, the simulation collects data). NLCommand(&quot;Setup&quot;) #runs the NetLogo setup command NLCommand(&quot;random-seed 4321&quot;) #sets the random number seed NLCommand(&quot;set surface_stability 0&quot;) #sets the surface_stability parameter NLDoCommand(2001,&quot;Go&quot;) #runs the model for 2000 iterations If you switch to the NetLogo GUI, you can see HMODEL operating. In the model, simulated agents move randomly from point to point within a gridded space, constructing hearths, shown as an X, at a constant rate. Hearths contain an “age” which records the date the hearth was formed in years before present. If nothing else were to happen, the record would show no change through time. Grid cells also contain a set of sedimentary layers, each of these also containing an age, with new hearths being constructed on the surface. At a given interval, an event will occur with one of two outcomes: erosion or deposition. If erosion occurs, the top layer of sediment erodes, any hearths situated on that surface lose their charcoal and become , while surfaces underneath become visible. If deposition occurs, a layer of sediment is added to the cell, and any hearths visible on the surface become hidden and thus undetectable in a surface survey. Diagram of HMODEL This simulation runs from 2000 BP to present, with hearth construction ceasing at 200 BP. At the end of the simulation, we can sample the hearths sitting on the surface and compare the chronologies we obtain from them. Hearths can be dated based on radiocarbon (for hearths containing charcoal), or optically stimulated luminsence (for hearth stones). The model keeps track of all hearths above and beneath the surface, as well as the intervening stratigraphic layers, so in effect the model can be “excavated”. 4.0.5 Getting data back from NetLogo First, we’ll use the ‘NLReport’ function to get lists of 100 randomly chosen hearths from three populations: visible hearths with charcoal that can be dated using radiocarbon, visible hearths with or without charcoal that can be dated using OSL, and the population of all hearths, visible or not. This last category should be completely unaffected by the model. hearth_c14&lt;- NLReport(&quot;[who] of n-of 100 hearths with [ hidden? = false and charcoal? = true ]&quot;) hearth_osl&lt;- NLReport(&quot;[who] of n-of 100 hearths with [ hidden? = false ]&quot;) hearth_null&lt;- NLReport(&quot;[who] of n-of 100 hearths&quot;) Next, we’ll create three empty tables, or dataframes, each with columns for hearth ID number, X and Y coordinates, and hearth age in years before present. c14_sample&lt;-data.frame(ID=integer(),X=double(),Y=double(),age=integer()) osl_sample&lt;-data.frame(ID=integer(),X=double(),Y=double(),age=integer()) null_sample&lt;-data.frame(ID=integer(),X=double(),Y=double(),age=integer()) Finally, we will go through each entry in our three lists of IDs using a for loop, pull the spatial and chronometric data listed above, and populate the tables with data from the simulated hearth sample. for (i in c(1:100)){ c14_sample[i,]&lt;-c(hearth_c14[i],NLReport(c(paste(&quot;[xcor] of hearth &quot;,hearth_c14[i]),paste(&quot;[ycor] of hearth &quot;,hearth_c14[i]),paste(&quot;[age] of hearth &quot;,hearth_c14[i])))) osl_sample[i,]&lt;-c(hearth_osl[i],NLReport(c(paste(&quot;[xcor] of hearth &quot;,hearth_osl[i]),paste(&quot;[ycor] of hearth &quot;,hearth_osl[i]),paste(&quot;[age] of hearth &quot;,hearth_osl[i])))) null_sample[i,]&lt;-c(hearth_null[i],NLReport(c(paste(&quot;[xcor] of hearth &quot;,hearth_null[i]),paste(&quot;[ycor] of hearth &quot;,hearth_null[i]),paste(&quot;[age] of hearth &quot;,hearth_null[i])))) } The result should be tables for each of three populations, each containing 100 hearths, their X and Y coordinates, and their age. For those who don’t have NetLogo installed, you can simply enter the following lines to read in the same data from comma-separated value files. c14_sample&lt;-read.csv(&quot;c14_hearths.csv&quot;,stringsAsFactors=FALSE) osl_sample&lt;-read.csv(&quot;osl_hearths.csv&quot;,stringsAsFactors=FALSE) null_sample&lt;-read.csv(&quot;null_hearths.csv&quot;,stringsAsFactors=FALSE) We can look at the first few entries in each table using the head function. For example: head(c14_sample) ## ID X Y age ## 1 7725 6.03502109 24.182946 456 ## 2 6804 11.08510667 4.779465 641 ## 3 8573 20.41160048 28.248093 287 ## 4 8662 25.97655397 20.660641 269 ## 5 5849 26.98930035 30.374603 832 ## 6 3928 -0.07997035 29.392092 1216 There is another (far simpler) way to do this using the RNetLogo extension, using the NLGetAgentSet function to obtain information from a subset of agents. For example: c14_sample2&lt;-NLGetAgentSet(c(&quot;who&quot;,&quot;xcor&quot;,&quot;ycor&quot;,&quot;age&quot;),&quot;n-of 100 hearths with [ hidden? = false and charcoal? = true]&quot;,as.data.frame=TRUE) However, the NetLogo software has recently undergone a substantial update, and this function in the RNetLogo package has come into conflict with the new version. I’ve been in touch with developer, who assures me this is being resolved. 4.0.6 Plotting the data To compare, we’ll plot the ages in chronological order by age. A record weighted toward the present should bend to the left, while a record weighted toward the past should bend to the right. plot(sort(null_sample$age),c(1:100),xlab=&quot;Years BP&quot;,ylab=&quot;Index by Age&quot;,main=&quot;&quot;,xlim=c(0,2000)) points(sort(osl_sample$age),c(1:100),pch=16,col=&quot;grey&quot;) points(sort(c14_sample$age),c(1:100),pch=16) abline(-11.11111111,0.055555555555555555555,lty=2) The diagonal dashed line shows what would be expected from a uniform record between 2000 and 200 BP. What this shows is the the process as modelled can produce the patterns of increasing frequency toward the present and pronounced gaps in a radiocarbon chronology (black dots), while at the same time showing a less pronounced increase and a lack of notable gaps in the OSL chronology (grey dots). The null distribution effectively follows the dashed line, as expected (unfilled dots). This is a point of difference from previous explanations. If the gaps in the radiocarbon record were an outcome of periodic human absence from the area, then no hearths should be constructed during the periods of absence; therefore, we should expect to see gaps in both the radiocarbon and OSL chronologies. However, the process in HMODEL can produce a chronometric record where gaps are present in the radiocarbon, but not the OSL. When these two proxies are compared with data from the field, specifically the Rutherfords Creek study area, we can see a similar trend, where conspicuous gaps in the C14 data are absent from the OSL data, casting doubt on the notion that the gaps are the result of changes in human occupation. Comparing C14 and OSL data (n=93) from Rutherfords Creek, New South Wales 4.0.7 Quitting NetLogo from R In order to quit this session of NetLogo, use the NLQuit function NLQuit() Keep in mind that once you have opened a NetLogo file using R, you are not able to open another in the same R session. You will need to restart the R session to open another model. sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] RNetLogo_1.0-3 igraph_1.0.1 rJava_0.9-8 ## ## loaded via a namespace (and not attached): ## [1] backports_1.0.5 bookdown_0.3.16 magrittr_1.5 rprojroot_1.2 ## [5] tools_3.3.3 htmltools_0.3.5 yaml_2.1.14 Rcpp_0.12.10 ## [9] stringi_1.1.3 rmarkdown_1.4 knitr_1.15.17 stringr_1.2.0 ## [13] digest_0.6.12 evaluate_0.10 "],
["tempo-plots-in-r-for-analysing-radiocarbon-dates.html", "Chapter 5 Tempo Plots in R for Analysing Radiocarbon Dates", " Chapter 5 Tempo Plots in R for Analysing Radiocarbon Dates Thomas S. Dye 5.0.1 Archaeological Motivation As archaeologists, we are interested in change over time, and as archaeological scientists we are interested in measuring it. The tempo plot is one way to measure change over time: it estimates the cumulative occurrence of archaeological events in a Bayesian calibration. The tempo plot yields a graphic where the slope of the plot directly reflects the pace of change: a period of rapid change yields a steep slope and a period of slow change yields a gentle slope. When there is no change, the plot is horizontal. When change is instantaneous, the plot is vertical. The example we’ll use in this demo includes radiocarbon dates collected beneath sixteen taro pond fields on three islands in Hawai`i. The dates provide termini post quos for the construction of the pond fields. In my work with the tempo plot so far I’ve interpreted different plot shapes as distinguishing tradition, innovation, and fashion. The tempo of change for the taro pond fields was interpreted as representing a tradition in old Hawai`i. 5.0.2 Obtain MCMC Output from Calibration Software A tempo plot summarizes the raw MCMC output yielded by a Bayesian calibration software package. A tempo plot can be constructed from the raw MCMC output produced by BCal, OxCal, and Chronomodel. Here, I’ll illustrate the use of OxCal. OxCal produces raw MCMC output when the MCMC_sample function appears near the start of the .oxcal file. OxCal’s MCMC_sample function takes three parameters: a name for the output file, the interval for sampling, and the maximum number of samples to write to the file. The example here will yield the file “loi-mcmc.csv” which will contain a maximum of 100,000 lines, using one in twenty-five of the MCMC iterations. MCMC_sample(&quot;loi-mcmc&quot;, 25, 100000) 5.0.3 Read OxCal MCMC Output Here, we call the read.csv function to read the raw MCMC output file produced by OxCal and assign it to the variable mcmc. This might take a while; the file loi-mcmc.csv is 69.4 MB. # read in the large CSV very quickly mcmc &lt;- readr::read_csv(&quot;loi-mcmc.csv&quot;) # convert colnames to read.csv style names(mcmc) &lt;- make.names(names(mcmc)) This line writes a list of the column names to standard output. We’ll use this to select the columns of interest. The list written to standard output shows the index of the left-most column names in square brackets. This is a convention in R. In this list, the first entry, Pass, refers to the iteration of the MCMC sampler. Subsequent entries 2 … 89 refer to various components of the Bayesian chronological model, including age determinations and phase boundaries. Most of this information is not useful for the tempo plot demonstration, so we’ll need to separate the wheat from the chaff. colnames(mcmc) ## [1] &quot;Pass&quot; &quot;Start.Pre.colonization&quot; ## [3] &quot;Beta.83313&quot; &quot;Wk.15982&quot; ## [5] &quot;Colonization&quot; &quot;KOU.CS.5a&quot; ## [7] &quot;Beta.20852b&quot; &quot;Wk.19312&quot; ## [9] &quot;Wk.19313&quot; &quot;Beta.135125&quot; ## [11] &quot;Beta.42172&quot; &quot;Beta.310824&quot; ## [13] &quot;Beta.343212&quot; &quot;Beta.5613&quot; ## [15] &quot;NOSAMS.0809.26&quot; &quot;Beta.339778&quot; ## [17] &quot;Beta.208143&quot; &quot;Beta.101871&quot; ## [19] &quot;Beta.101872&quot; &quot;Beta.260904&quot; ## [21] &quot;Beta.138980&quot; &quot;Beta.150615&quot; ## [23] &quot;Beta.233042&quot; &quot;Beta.150620&quot; ## [25] &quot;Beta.251247&quot; &quot;Wk.19311&quot; ## [27] &quot;Wk.19310&quot; &quot;Beta.135126&quot; ## [29] &quot;Beta.260905&quot; &quot;Beta.45363&quot; ## [31] &quot;Beta.28136&quot; &quot;Beta.28135&quot; ## [33] &quot;CAMS.25560&quot; &quot;CAMS.26396&quot; ## [35] &quot;CAMS.25561&quot; &quot;SR.5081&quot; ## [37] &quot;SR.5085&quot; &quot;SR.5080&quot; ## [39] &quot;SR.5082&quot; &quot;End.Post.colonization&quot; ## [41] &quot;Anahulu.start&quot; &quot;X...&quot; ## [43] &quot;O.Anahulu&quot; &quot;Lower.Elialii.start&quot; ## [45] &quot;Beta.213276&quot; &quot;M.Lower.Elialii&quot; ## [47] &quot;Upper.Elialii.start&quot; &quot;Beta.213274&quot; ## [49] &quot;M.Upper.Elialii&quot; &quot;Kuele.central.2.start&quot; ## [51] &quot;AA.72543&quot; &quot;M.Kuele.central.2&quot; ## [53] &quot;Halawa.stage.1.start&quot; &quot;Beta.263862&quot; ## [55] &quot;H.Halawa.stage.1&quot; &quot;Halepoki.makai.start&quot; ## [57] &quot;AA71542b&quot; &quot;M.Halepoki.makai&quot; ## [59] &quot;Kuele.central.1.start&quot; &quot;AA.71549&quot; ## [61] &quot;M.Kuele.central.1&quot; &quot;Halawa.bund.start&quot; ## [63] &quot;Beta.263861&quot; &quot;H.Halawa.bund&quot; ## [65] &quot;Kukuinui.start&quot; &quot;AA71541&quot; ## [67] &quot;M.Kukuinui&quot; &quot;Kuele.west.start&quot; ## [69] &quot;AA71122&quot; &quot;M.Kuele.west&quot; ## [71] &quot;Halepoki.central.start&quot; &quot;AA71550&quot; ## [73] &quot;M.Halepoki.central&quot; &quot;Keiu.start&quot; ## [75] &quot;Beta.193986&quot; &quot;M.Keiu&quot; ## [77] &quot;Halepoki.mauka.start&quot; &quot;AA72162&quot; ## [79] &quot;M.Halepoki.mauka&quot; &quot;Pawaa.central.start&quot; ## [81] &quot;AA71121&quot; &quot;M.Pawaa.central&quot; ## [83] &quot;Pawaa.makai.start&quot; &quot;AA72161&quot; ## [85] &quot;M.Pawaa.makai&quot; &quot;Lahokea.start&quot; ## [87] &quot;Beta.215407&quot; &quot;M.Lahokea&quot; ## [89] &quot;X89&quot; 5.0.4 Select and Confirm Columns to Plot For this example, we are interested in construction date estimates for taro pond fields. In the Bayesian chronological model these estimates are labeled starting with a capital letter followed by a dot. Indices of the construction date estimates are assigned to the variable i. From the output of colnames(mcmc) above, we can see that column 43 is named “O.Anahulu”, which represents the construction date estimate of a taro pond field in the Anahulu Valley of O`ahu Island, column 46 is named “M.Lower.Elialii”, which represents the construction date estimate of a taro pond field in the lower Elialii system in a valley on Moloka`i Island, etc. i &lt;- c(43, 46, 49, 52, 55, 58, 61, 64, 67, 70, 73, 76, 79, 82, 85, 88) The interesting portion of mcmc is assigned to the variable mcmc.select, using the indexing facility of R. The square brackets at the end of this line enclose two values—a nil value before the comma, and the vector of indices in our variable i after the comma—that instruct R to select all rows but only the columns indexed in i. mcmc.select &lt;- mcmc[,i] We now have a table with 16 columns, instead of one with 89 columns. Here, we check that the column names of mcmc.select are the ones we intended to select. Happily, there are 16 of them and they appear to be correct. The column named “O.Anahulu” that was the 43rd column of mcmc is now the first column of mcmc.select. colnames(mcmc.select) ## [1] &quot;O.Anahulu&quot; &quot;M.Lower.Elialii&quot; &quot;M.Upper.Elialii&quot; ## [4] &quot;M.Kuele.central.2&quot; &quot;H.Halawa.stage.1&quot; &quot;M.Halepoki.makai&quot; ## [7] &quot;M.Kuele.central.1&quot; &quot;H.Halawa.bund&quot; &quot;M.Kukuinui&quot; ## [10] &quot;M.Kuele.west&quot; &quot;M.Halepoki.central&quot; &quot;M.Keiu&quot; ## [13] &quot;M.Halepoki.mauka&quot; &quot;M.Pawaa.central&quot; &quot;M.Pawaa.makai&quot; ## [16] &quot;M.Lahokea&quot; 5.0.5 Load and Run calc.tempo Function This line reads in the tempo plot source code distributed in the file tempo-plot-demo-saa-2017.r. There are two functions in the file, which we’ll use to calculate the tempo plot and to print it to a graphics device. source(&quot;tempo-plot-demo-saa-2017.r&quot;) Here is the calc.tempo function we’ll use to calculate the tempo plot. It is worth looking at in detail because other joint posterior calculations will have similar structure. The function has three parameters: mcmc.data is the selected columns of the raw MCMC output from OxCal by.int specifies the interval in years between points for which the statistic will be calculated out.file is an optional parameter that will write the tempo calculation to a file. The first line finds the minimum and maximum values in mcmc.data and creates a vector from the minimum to the maximum by the interval by.int. The vector is assigned to the variable years; these are the years for which the statistic will be calculated. The second line declares a matrix res.mat large enough to hold the results of the calculation. Most of the work takes place in the for loop, which loops over years and determines for each of the 100,000 lines in mcmc.data how many taro pond fields were constructed before that year. This is the part of the function that would change if a different statistic were implemented. The results of the simulation are summarized in the lines assigning values to the variables means and sds. The results are packaged up in a dataframe assigned to the variable res.df. The variable res.df is then written to a file if the out.file parameter has a value and finally returned. calc.tempo &lt;- function(mcmc.data, by.int, out.file=&quot;&quot;) { years &lt;- seq(from = floor(min(mcmc.data)), to = ceiling(max(mcmc.data)), by = by.int) res.mat &lt;- matrix(ncol = length(years), nrow = dim(mcmc.data)[1]) for (i in 1:length(years)) { gte &lt;- mcmc.data &lt;= years[i] res.mat[,i] &lt;- rowSums(gte) } means &lt;- colMeans(res.mat) sds &lt;- apply(res.mat, 2, sd) res.df &lt;- data.frame(mean = means, sd = sds, year = years) if (!(out.file == &quot;&quot;)) { write.csv(res.df, out.file) } return(res.df) } Here, we call calc.tempo with our mcmc.select data, ask the function to calculate the statistic by decade, and save the results to a file, loi-tempo.csv. In addition, we save the results in a variable, loi.tempo, to save ourselves the trouble of reading them back in from the file. If, at some later date, we desire access to the data, then we can read.csv(&quot;loi-tempo.csv&quot;). loi.tempo &lt;- calc.tempo(mcmc.data = mcmc.select, by.in = 10, out.file = &quot;loi-tempo.csv&quot;) 5.0.6 Check calc.tempo Output A summary of the calc.tempo results suggests all went well. The mean number of construction events ranges from 0 to just under 16, the number of taro pond fields in our sample. The calculations were carried out for a period of almost 900 years, from AD 1056 to 1946. summary(loi.tempo) ## mean sd year ## Min. : 0.0000 Min. :0.0000 Min. :1056 ## 1st Qu.: 0.3658 1st Qu.:0.4707 1st Qu.:1278 ## Median : 5.7018 Median :1.4603 Median :1501 ## Mean : 5.6574 Mean :1.0657 Mean :1501 ## 3rd Qu.: 9.0754 3rd Qu.:1.5279 3rd Qu.:1724 ## Max. :15.8329 Max. :1.5576 Max. :1946 5.0.7 Example Plot with plot.tempo The source file, tempo-plot-demo-saa-2017.r, includes a simple function to plot the calc.tempo results based on the ggplot2 library. The function plot.tempo accepts data from an R variable with the parameter tempo.data or from a file created by calc.tempo with the in.file parameter. If both tempo.data and in.file are supplied, then the data are taken from tempo.data and the in.file parameter is ignored. The plot.tempo function displays the graph on-screen. If the parameter out.file is specified with a graphics file extension recognized by R, then the graphic will be written to out.file. If out.file is not specified, then only the on-screen graph is produced. Some parameters modify the graphic output. The parameters min.x and max.x can be used to specify the x-axis limits, so you can “zoom” in or out. The parameters x.label and y.label make it possible to modify the x-axis and y-axis labels. The parameters plot.ht and plot.wi set the height and width of the plot in inches. Their default values are standard R values. plot.tempo &lt;- function(tempo.data = NULL, in.file = &quot;&quot;, out.file = &quot;&quot;, max.x = NA, min.x = NA, y.label = &quot;Cumulative Events&quot;, x.label = &quot;Calendar Year&quot;, plot.ht = 7, plot.wi = 7){ library(ggplot2) if (is.null(tempo.data)){ if (in.file == &quot;&quot;){ stop(&quot;No data source&quot;)} else { tempo.data &lt;- read.csv(in.file)} } h &lt;- ggplot(tempo.data, aes(x = year)) h &lt;- h + geom_ribbon(aes(ymin = mean - sd, ymax = mean + sd)) h &lt;- h + geom_line(aes(y = mean)) h &lt;- h + xlab(x.label) + ylab(y.label) if (!(is.na(max.x) | is.na(min.x))){ h &lt;- h + xlim(min.x, max.x) } if (!(out.file == &quot;&quot;)) { ggsave(filename = out.file, plot= h, height = plot.ht, width = plot.wi) } old.par &lt;- par(no.readonly = T) dev.new(width = plot.wi, height = plot.ht) print(h) par(old.par) } Here, the tempo.data function is called with the variable loi.tempo produced by the function calc.tempo and the graph is saved in PDF format to the file loi-tempo.png. The plot height is set to 3 inches, so both the on-screen graphic and the graphic saved to loi-tempo.png will be short and wide. plot.tempo(tempo.data = loi.tempo, out.file = &quot;loi-tempo.png&quot;, plot.ht = 3) The steady tempo of taro pond field construction events over a period of more than 600 years indicates that the practice was a tradition in old Hawai`i. 5.0.8 ArchaeoPhases software Developers of the Bayesian calibration application Chronomodel, Anne Philippe and Marie-Ann Vibet at Jean Leray Mathematics Lab, Université de Nantes, are developing open-source R software called ArchaeoPhases to process the raw MCMC output from Chronomodel and OxCal. Support for the raw MCMC output of BCal is planned. The ArchaeoPhases software includes a function, TempoPlot, that develops a Bayesian implementation of the tempo plot. Here is an example of graphical output using code currently under development and expected to be included in the next release of the software. 5.0.9 Demo Files The demo comprises these four files: loi-mcmc.csv, a large comma-separated value file containing the raw MCMC data from an OxCal calibration; tempo-plot-demo-saa-2017.r, an R source code file with definitions of the functions calc.tempo and plot.tempo; and tempo-plot-demo-saa-2017.Rmd, the R Markdown source for the document you are reading. loi-ap.png, a graphics file produced by ArchaeoPhases software under development. These two files are produced during the demo: loi-tempo.csv, a comma-separated value file containing the results returned by the calc.tempo function; and loi-tempo.png, a png file of the graphic produced by the function plot.tempo. sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] ggplot2_2.2.1 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.10 knitr_1.15.17 magrittr_1.5 hms_0.3 ## [5] munsell_0.4.3 colorspace_1.3-2 R6_2.2.0 stringr_1.2.0 ## [9] plyr_1.8.4 tools_3.3.3 grid_3.3.3 gtable_0.2.0 ## [13] htmltools_0.3.5 yaml_2.1.14 lazyeval_0.2.0 rprojroot_1.2 ## [17] digest_0.6.12 assertthat_0.1 tibble_1.2 bookdown_0.3.16 ## [21] readr_1.1.0 evaluate_0.10 rmarkdown_1.4 labeling_0.3 ## [25] stringi_1.1.3 methods_3.3.3 scales_0.4.1 backports_1.0.5 "],
["using-r-as-a-gis-working-with-raster-and-vector-data.html", "Chapter 6 Using R as a GIS: working with raster and vector data", " Chapter 6 Using R as a GIS: working with raster and vector data Daniel Contreras 6.0.1 Introduction A common GIS task in archaeology is that of relating raster and vector data - e.g., relating site locations to environmental variables such as elevation, slope, and aspect. It’s common to do this by calculating values for the point locations in a shapefile of sites, and often of interest to compare environmental variables across some aspect of site variability - function, time period, size, etc. Here we’ll take a slightly more robust approach that looks at the areas around sites rather than their precise locations, calculating a buffer around each site location and then averaging the values of multiple environmental variables within those buffers. For simplicity we’ll use a DEM and rasters of slope and aspect that we can derive from it, but any other raster data could also be employed. Using a fictionalized dataset for a small area in Provence, we’ll take a look at some tools for exploring spatial data, and show that the areas settled in the Early Iron Age and the Gallo-Roman Period were significantly distinct with respect to some variables, but similar in others. We might hypothesize, for instance, that the Roman colonization of the area was based on cereal agriculture production for export, and that as a result Gallo-Roman period settlement would have prioritized relatively low, flat land appropriate for such activity. We could test this hypothesis by comparing the areas occupied in the two periods with respect to elevation and slope. 6.0.1.1 First Steps: Data Import and Format We’ll begin by loading a few packages… # getting started: load necessary packages require(rgdal) require(raster) require(rasterVis) require(lattice) …and then importing and examining the data. # Import data - first a 30m DEM from NASA SRTM data (http://doi.org/10.5067/MEaSUREs/SRTM/SRTMGL1N.003) in geotiff format (though raster() will read any format that rgdal::readGDAL() recognizes), and then a point shapefile (a standard ESRI .shp, though rgdal::readOGR() can import many other sorts of spatial data also). # commands: raster::raster(), rgdal::readOGR() areaDEM &lt;- raster(&quot;demo_files/areaDEM.tif&quot;) # read raster areaDEMutm &lt;- projectRaster(areaDEM, crs=&quot;+proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # It will be easier to work with projected data, so we&#39;ll project this to UTM using the appropriate proj4 string (http://proj4.org/index.html) for the CRS (Coordinate Reference System) that we want. areaDEMutm # Have a quick look at this to make sure nothing has gone terribly wrong - with the raster package loaded typing the name of a raster object will give you summary data about that object. ## class : RasterLayer ## dimensions : 862, 1372, 1182664 (nrow, ncol, ncell) ## resolution : 22.3, 30.9 (x, y) ## extent : 670963.1, 701558.7, 4838878, 4865514 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## data source : in memory ## names : areaDEM ## values : 91.62182, 1121.591 (min, max) sites &lt;- readOGR(dsn=&quot;demo_files&quot;, layer=&quot;areaPoints&quot;) # read .shp (note that to read a shapefile, &quot;the data source name (dsn= argument) is the folder (directory) where the shapefile is, and the layer is the name of the shapefile (without the .shp extension)&quot; (from the rgdal::readOGR documentation)) ## OGR data source with driver: ESRI Shapefile ## Source: &quot;demo_files&quot;, layer: &quot;areaPoints&quot; ## with 1061 features ## It has 3 fields ## Integer64 fields read as strings: ID sites_sub &lt;- sites[sites$period == &quot;EIA&quot; | sites$period == &quot;GalRom&quot;,] # subset points to eliminate sites of uncertain date - i.e., select from &#39;sites&#39; only those rows in which the &#39;period&#39; column is &quot;EIA&quot; or &quot;GalRom&quot;. sites_sub$period &lt;- factor(sites_sub$period) # drop unused levels (not strictly necessary but will avoid messiness when plotting data later) sites_sub_utm &lt;- spTransform(sites_sub, &quot;+proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0&quot;) # project points to UTM sites_sub_utm # Check the file (note that it is now a Spatial Points Data Frame, and typing its name will give you an object summary). Note that there is a &#39;type&#39; field that we won&#39;t work with here, but which could be incorporated into this kind of analysis, e.g., by further sub-setting or by grouping data when boxplotting. ## class : SpatialPointsDataFrame ## features : 585 ## extent : 671788.9, 700793.6, 4839110, 4864963 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## variables : 3 ## names : period, type, ID ## min values : EIA, domestic, 1 ## max values : GalRom, other, 99 6.0.1.2 Working with a DEM We can use the DEM to calculate DEM derivatives. # # commands: raster::terrain() area_slope &lt;- terrain(areaDEMutm, opt = &#39;slope&#39;, unit = &#39;degrees&#39;) #calculate slope area_aspect &lt;- terrain(areaDEMutm, opt = &#39;aspect&#39;, unit = &#39;degrees&#39;) #calculate aspect #Have a quick look at these to see that the results make sense - they are now raster objects just like areaDEM and can be examined the same way. area_slope ## class : RasterLayer ## dimensions : 862, 1372, 1182664 (nrow, ncol, ncell) ## resolution : 22.3, 30.9 (x, y) ## extent : 670963.1, 701558.7, 4838878, 4865514 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## data source : in memory ## names : slope ## values : 0, 64.11368 (min, max) area_aspect ## class : RasterLayer ## dimensions : 862, 1372, 1182664 (nrow, ncol, ncell) ## resolution : 22.3, 30.9 (x, y) ## extent : 670963.1, 701558.7, 4838878, 4865514 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## data source : in memory ## names : aspect ## values : 0, 360 (min, max) Those summaries are useful, but let’s also plot our raster DEM; this is not strictly necessary, but it’s reassuring to see that the data are meaningful. We won’t try to make the plot too pretty; the options specified do just enough to plot it as an intelligible terrain DEM. 6.0.1.3 Raster and Point Plotting # # commands: rasterVis::levelplot(), it&#39;s also a function in the lattice pkg, so let&#39;s specify the namespace just to be sure we are using the right function rasterVis::levelplot(areaDEMutm, margin = list(x = FALSE, y = TRUE), col.regions = terrain.colors(16), xlab = list(label = &quot;&quot;, vjust = -0.25), sub = list( label = &quot;masl&quot;, font = 1, cex = .9, hjust = 1.5)) We can check that our data occupy the same space, and visually assess our hypothesis, by plotting the raster with the sites overlain as points, varying color and shape by period. Since this is just for exploratory purposes, we won’t worry about making it too pretty. # commands: sp::spplot() rasterVis::levelplot(areaDEMutm, margin = list(x = F, y = T), col.regions = terrain.colors(16), xlab = list (label = &quot;&quot;, vjust = -.25), sub = list( label = &quot;masl&quot;, font = 1, cex = .9, hjust = 1.5), key = list( #this time we&#39;ll include a legend that identifies the points we&#39;ll plot space = &quot;top&quot;, points = list( pch = c(18,20), col = c(&quot;red&quot;,&quot;blue&quot;)), text = list( c(&quot;EIA&quot;,&quot;GalRom&quot;), cex=.8)) ) + spplot(sites_sub_utm, # add a layer of points zcol = &quot;period&quot;, cex = .6, pch = c(18,20), col.regions = c(&quot;red&quot;,&quot;blue&quot;) ) Satisfied that our data make sense, and perhaps already having developed some ideas about site distribution just by staring at the map, we can move on to explore the site locations analytically. We’ll begin by building a raster stack that consists of the rasters of the three variables we’ll examine: elevation, slope, and aspect. 6.0.1.4 Raster Stacks # commands: raster::stack() terrainstack &lt;- stack(areaDEMutm, area_slope, area_aspect) terrainstack # have a quick look at resulting object, which shows the number of layers and the min/max values we expect ## class : RasterStack ## dimensions : 862, 1372, 1182664, 3 (nrow, ncol, ncell, nlayers) ## resolution : 22.3, 30.9 (x, y) ## extent : 670963.1, 701558.7, 4838878, 4865514 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=utm +zone=31 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0 ## names : areaDEM, slope, aspect ## min values : 91.62182, 0.00000, 0.00000 ## max values : 1121.59056, 64.11368, 359.99998 6.0.1.5 Using extract() to selectively summarize raster data We can then take that raster stack and use extract() to summarize values of DEM, slope, and aspect within a buffer around each point. # commands: raster::extract() sites_vals &lt;- extract(terrainstack, sites_sub_utm, buffer = 250, fun = mean, sp = TRUE) # extract the mean values w/in a 250m radius around each site for each terrain variable The results can be examined in any number of ways; here we’ll make boxplots for each of the variables of interest. We’ll plot each variable by period, so each result will comprise two box-and-whisker plots, one for the Early Iron Age and one for the Gallo-Roman Period 6.0.1.6 Boxplotting for exploratory data analysis As with the exploratory raster-and-point plotting we did above, we won’t worry too much about making these plots pretty; we’ll just do enough to make them easily visually intelligible. I’m using the lattice package for the plotting, but there are several other ways to do this as well. Something that we won’t get into here because it adds a layer of complication is to also characterize the environmental background - i.e., to assess the question of whether settlement locations comprise a random sample of the the available landscape or instead focus on certain kinds of locations. # commands: summary(), lattice::bwplot() summary(sites_vals$period)#check the sample size for each period (I&#39;ve done this and manually incorporated it in the boxplot labels) ## EIA GalRom ## 94 491 elevplot &lt;- bwplot(areaDEM ~ period, # Here we&#39;re writing the boxplot to an object for later use (if you just want to display it, simply run the code without writing to an object) data = data.frame(sites_vals), notch = TRUE, pch = &quot;|&quot;, fill = &quot;grey&quot;, box.ratio = 0.25, par.settings = list( box.rectangle = list( col = c(&quot;red&quot;,&quot;blue&quot;))), #to maintain a visual link to our map, we&#39;ll plot the box outlines with the same color scheme ylab = &quot;masl&quot;, main=&quot;Elevation&quot;, scales = list(x = list(labels = c(&quot;Early Iron Age\\n(n = 94)&quot;, &quot;Gallo-Roman\\n(n = 491)&quot;)), rot=60)) elevplot #examine the result to make sure everything is in order #repeat for slope slopeplot &lt;- bwplot(slope ~ period, data = data.frame(sites_vals), notch = TRUE, pch = &quot;|&quot;, fill = &quot;grey&quot;, box.ratio = 0.25, par.settings = list( box.rectangle = list( col = c(&quot;red&quot;,&quot;blue&quot;))), ylab = &quot;slope (degrees)&quot;, main = &quot;Slope&quot;, scales = list(x = list(labels = c(&quot;Early Iron Age\\n(n = 94)&quot;, &quot;Gallo-Roman\\n(n = 491)&quot;)), rot = 60)) # view the plot slopeplot #and then aspect aspectplot &lt;- bwplot(aspect ~ period, data = data.frame(sites_vals), notch = TRUE, pch = &quot;|&quot;, fill = &quot;grey&quot;, box.ratio = 0.25, par.settings = list( box.rectangle = list( col = c(&quot;red&quot;,&quot;blue&quot;))), ylab = &quot;aspect (degrees)&quot;, main = &quot;Aspect&quot;, scales = list(x = list(labels = c(&quot;Early Iron Age\\n(n = 94)&quot;, &quot;Gallo-Roman\\n(n = 491)&quot;)), rot=60)) # view the plot aspectplot We can already assess our hypothesis by looking at those plots individually, but since we’ve written them to objects we can also easily juxtapose them in a single plot and consider the results. # a bonus plotting problem: putting all three of these in one plot #commands: gridExtra::grid.arrange require(gridExtra) grid.arrange(elevplot, slopeplot, aspectplot, nrow = 1, ncol = 3) We can now quantitatively assess whatever visual assessments we made: it’s clear (the notches give a rough 95% confidence interval for the medians) that parts of the landscape of differing slope and elvation were preferred in the two periods (and, in contrast, that aspect was considered similarly or perhaps remained unimportant). 6.0.1.7 Summary What does all this mean for the archaeological question?Our hypothesis looks pretty compelling, at first glance. There is a marked change between the Early Iron Age and the Gallo-Roman Period in the parts of the landscape preferred, and the pattern is robust. That said, this should be seen as exploratory analysis, and common GIS caveats apply: the variables we’re looking at hardly comprise an exhaustive list, there’s perhaps some danger of ecological fallacy with categories so broad, there’s always some risk that some other variable that is related to - or at least covaries with - those we’ve examined is actually the causal one, issues like data quality and landscape taphonomy ought to be considered, we are privileging environmental variables because that data is easily available, etc. However, those are problems of argument construction more than of method, and I find this a powerful set of tools with which to explore data and think about which arguments to test in more detail. Such issues might be addressed by further exploring the data (e.g., exploring other site variables, varying the buffer size, varying the summary statistics use to describe the landscape within the buffer), adding other data (e.g., rasters of distance-from-streams, soil type and depth, etc.), and thinking about what it would take to build a more robust argument. Finally, by way of considering what we might gain by using R as a GIS in this way (rather than QGIS, ArcGIS, etc) I want to highlight four of the steps we’ve gone through: subsetting data adding environmental (or any spatial) variables as raster data and summarizing them with respect to site locations specifying analytical parameters (e.g., buffer size, summary statistics) visualising the distributions of environmental variables for the sites All of these provide flexibility in the particulars of the analysis, and the entire process is easily repeatable in varying permutations or with different data. sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] gridExtra_2.2.1 rasterVis_0.41 latticeExtra_0.6-28 ## [4] RColorBrewer_1.1-2 lattice_0.20-34 raster_2.5-8 ## [7] rgdal_1.2-5 sp_1.2-4 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.10 knitr_1.15.17 magrittr_1.5 ## [4] viridisLite_0.2.0 stringr_1.2.0 tools_3.3.3 ## [7] parallel_3.3.3 grid_3.3.3 gtable_0.2.0 ## [10] htmltools_0.3.5 yaml_2.1.14 rprojroot_1.2 ## [13] digest_0.6.12 bookdown_0.3.16 evaluate_0.10 ## [16] rmarkdown_1.4 stringi_1.1.3 backports_1.0.5 ## [19] hexbin_1.27.1 zoo_1.7-14 "],
["using-r-to-analyse-occupation-as-a-function-of-geographical-variables-using-logistic-regression-.html", "Chapter 7 Using R to analyse occupation as a function of geographical variables using logistic regression. 7.1 Introduction", " Chapter 7 Using R to analyse occupation as a function of geographical variables using logistic regression. Ariane Burke 7.1 Introduction This demonstration is based on research published in: Burke, Ariane, et al. 2014 Exploring the impact of climate variability during the Last Glacial Maximum on the pattern of human occupation of Iberia. Journal of Human Evolution 73:35-46. http://dx.doi.org/10.1016/j.jhevol.2014.06.003 ## ----- Install packages require(car) ## for vif (variance inflation test) # BM # we can install coefplot2 from: # devtools::install_github(&quot;palday/coefplot2&quot;, subdir = &quot;pkg&quot;) # or # install.packages(&quot;coefplot2&quot;,repos=&quot;http://www.math.mcmaster.ca/bolker/R&quot;, type=&quot;source&quot;) require(AICcmodavg) ## for AIC model selection require(MASS) ## for stepAIC library(coefplot2) ## for coefficient plot. ## Set file name fileName &lt;- &quot;Test_Iberia.csv&quot; ## ----- Read in data dat &lt;- read.csv(fileName, header=T) ## ----- View/check data head(dat) ## Name LGM X Y t_avg_y t_min_y slope elev p_avg_y ## 1 Abauntz 1 -1.6333 43.0015 3.82 -8.2 8.27 241 64.13 ## 2 Aitzbitarte IV 1 -1.8956 43.2628 6.05 -6.2 3.37 493 50.40 ## 3 Altamira 1 -4.1190 43.3780 6.30 -5.1 8.06 92 59.89 ## 4 Amalda 1 -2.2047 43.2342 5.31 -6.6 26.16 112 60.40 ## 5 Ambrosio 1 -2.0991 37.8222 8.88 -3.1 7.25 85 61.86 ## 6 Arbreda 1 2.7468 42.1618 7.58 -5.4 4.31 83 55.08 ## p_min_spr ## 1 30.34 ## 2 24.92 ## 3 31.76 ## 4 29.76 ## 5 34.24 ## 6 27.64 library(ggplot2) library(viridis) map_elev &lt;- ggplot(dat, aes(X, Y, colour = elev)) + geom_point(size = 2, shape = 15) + scale_color_viridis() + theme_minimal() + coord_equal() + ggtitle(&quot;Study area with elevation&quot;) map_precip &lt;- ggplot(dat, aes(X, Y, colour = p_min_spr)) + geom_point(size = 2, shape = 15) + scale_color_viridis(option = &quot;C&quot;) + theme_minimal() + coord_equal() + ggtitle(&quot;Study area with precipitation&quot;) library(gridExtra) grid.arrange(map_elev, map_precip) ### ----- Standardization of predictors dat1 &lt;- as.data.frame(cbind(dat[,c(1:4)], (scale(dat[,-c(1:4)])))) #dat2&lt;- dat2[,] ## removal of extra non informative variables # Verification (make sure the vars are numerical) str(dat1) ## &#39;data.frame&#39;: 2214 obs. of 10 variables: ## $ Name : Factor w/ 2214 levels &quot;1&quot;,&quot;10&quot;,&quot;100&quot;,..: 2180 2181 2182 2183 2184 2185 2186 2187 2188 2189 ... ## $ LGM : int 1 1 1 1 1 1 1 1 1 1 ... ## $ X : num -1.63 -1.9 -4.12 -2.2 -2.1 ... ## $ Y : num 43 43.3 43.4 43.2 37.8 ... ## $ t_avg_y : num -1.136 -0.307 -0.214 -0.582 0.746 ... ## $ t_min_y : num -1.195 -0.58 -0.242 -0.703 0.372 ... ## $ slope : num 0.484 -0.351 0.448 3.532 0.31 ... ## $ elev : num -0.85 -0.206 -1.231 -1.18 -1.249 ... ## $ p_avg_y : num 0.1113 -1.0793 -0.2564 -0.2122 -0.0856 ... ## $ p_min_spr: num -0.178 -0.9476 0.0237 -0.2603 0.3758 ... Using all of the data including the zeros, which cannot be described as necessarily “true” zeroes, we introduce an artificial level of certainty regarding absences. This can bias results (King and Zeng, 2000; Dixon et al.,2005), (also see http://www2.unil.ch/biomapper/Download/Chefaoui-EcoMod-2008.pdf) To reduce this uncertainy, we sample the zeros which we now call pseudo absences. Good practice is 10x the number of presences (=1000). We can bootstrap this if we want, but this 1000 samples should be sufficient. We will nonetheless need to report some sensitivity to the number of absences used. ## ----- Tease apart presences and absences, then select a subset of absences - recombine presences (all) and absences (sample) numAbsences &lt;- 350 ## 10x presences Presences &lt;- subset(dat1, dat1$LGM == 1) Absences &lt;- subset(dat1, dat1$LGM == 0)[sample(c(1:dim(dat1)[1]), numAbsences),] ## ----- Final data on which analyses will be run dat2 &lt;- rbind(Presences, Absences) ## ----- data table for forward selection (stepAIC) dat3 &lt;- dat2[,c(2, 5, 6, 7, 8, 9, 10)] ## ----- View/check data head(dat3) ## LGM t_avg_y t_min_y slope elev p_avg_y p_min_spr ## 1 1 -1.1364478 -1.1949906 0.4840559 -0.8504833 0.11126454 -0.17797971 ## 2 1 -0.3069777 -0.5803098 -0.3508109 -0.2063003 -1.07927238 -0.94763063 ## 3 1 -0.2139877 -0.2422353 0.4482759 -1.2313694 -0.25638852 0.02366315 ## 4 1 -0.5822278 -0.7032459 3.5321719 -1.1802437 -0.21216610 -0.26034088 ## 5 1 0.7456684 0.3724455 0.3102673 -1.2492633 -0.08556859 0.37582815 ## 6 1 0.2621207 -0.3344375 -0.1906528 -1.2543759 -0.67346664 -0.56138515 ## ----- MODELLING ## ----- Define formulae form0 &lt;- formula(LGM ~ 1) #intercept only model form1 &lt;- formula(LGM ~ Y + X) form2 &lt;- formula(LGM ~ elev + slope) form3 &lt;- formula(LGM ~ t_min_y) form4 &lt;- formula(LGM ~ elev + slope + p_min_spr) form5 &lt;- formula(LGM ~ p_min_spr + t_min_y) form6 &lt;- formula(LGM ~ t_avg_y + p_avg_y) form7 &lt;- formula(LGM ~ elev + slope + t_avg_y + p_avg_y) form8 &lt;- formula(LGM~ .) ## all variables for step-wise procedure ## all variables for step-wise procedure ## ----- Build models mod.0 &lt;- glm(form0, family = binomial, data = dat2) mod.1 &lt;- glm(form1, family = binomial, data = dat2) mod.2 &lt;- glm(form2, family = binomial, data = dat2) mod.3 &lt;- glm(form3, family = binomial, data = dat2) mod.4 &lt;- glm(form4, family = binomial, data = dat2) mod.5 &lt;- glm(form5, family = binomial, data = dat2) mod.6 &lt;- glm(form6, family = binomial, data = dat2) mod.7 &lt;- glm(form7, family = binomial, data = dat2) mod.8 &lt;- stepAIC(glm(form8, family = binomial, data = dat3)) ## Start: AIC=183.98 ## LGM ~ t_avg_y + t_min_y + slope + elev + p_avg_y + p_min_spr ## ## Df Deviance AIC ## - t_min_y 1 170.06 182.06 ## - t_avg_y 1 170.14 182.14 ## &lt;none&gt; 169.98 183.98 ## - slope 1 182.22 194.22 ## - p_avg_y 1 185.86 197.86 ## - p_min_spr 1 188.61 200.61 ## - elev 1 196.79 208.79 ## ## Step: AIC=182.06 ## LGM ~ t_avg_y + slope + elev + p_avg_y + p_min_spr ## ## Df Deviance AIC ## - t_avg_y 1 170.28 180.28 ## &lt;none&gt; 170.06 182.06 ## - slope 1 182.77 192.77 ## - p_avg_y 1 188.09 198.09 ## - p_min_spr 1 188.61 198.61 ## - elev 1 196.97 206.97 ## ## Step: AIC=180.28 ## LGM ~ slope + elev + p_avg_y + p_min_spr ## ## Df Deviance AIC ## &lt;none&gt; 170.28 180.28 ## - slope 1 183.47 191.47 ## - p_min_spr 1 188.98 196.98 ## - p_avg_y 1 189.04 197.04 ## - elev 1 197.13 205.13 ## ----- Summarize AIC results, including weightings. Using modaicavg package. mods &lt;- list(mod.0, mod.1, mod.2, mod.3, mod.4, mod.5, mod.6, mod.7, mod.8) modnames &lt;- c(&quot;mod.0&quot;, &quot;mod.1&quot;, &quot;mod.2&quot;, &quot;mod.3&quot;, &quot;mod.4&quot;, &quot;mod.5&quot;, &quot;mod.6&quot;, &quot;mod.7&quot;, &quot;mod.8&quot;) aictab(mods, modnames, second.ord = T) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## mod.8 5 180.45 0.00 1 1 -85.14 ## mod.2 3 195.89 15.44 0 1 -94.91 ## mod.4 4 197.14 16.70 0 1 -94.52 ## mod.7 5 198.77 18.32 0 1 -94.30 ## mod.1 3 224.91 44.46 0 1 -109.42 ## mod.0 1 235.62 55.18 0 1 -116.81 ## mod.3 2 237.28 56.84 0 1 -116.63 ## mod.5 3 237.96 57.51 0 1 -115.95 ## mod.6 3 238.49 58.05 0 1 -116.22 summary(mod.8) ## ## Call: ## glm(formula = LGM ~ slope + elev + p_avg_y + p_min_spr, family = binomial, ## data = dat3) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.16010 -0.43006 -0.19391 -0.08459 2.93785 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.7096 0.4344 -8.539 &lt; 2e-16 *** ## slope 0.5945 0.1629 3.650 0.000262 *** ## elev -1.5847 0.3889 -4.075 4.61e-05 *** ## p_avg_y -2.1806 0.5455 -3.997 6.40e-05 *** ## p_min_spr 2.1741 0.5265 4.129 3.64e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 233.61 on 379 degrees of freedom ## Residual deviance: 170.28 on 375 degrees of freedom ## (5 observations deleted due to missingness) ## AIC: 180.28 ## ## Number of Fisher Scoring iterations: 7 ## ---- Coefficient Plot for model coefplot2(mod.8, main = &quot;Model 8&quot;, col = &#39;blue&#39;, cex.pts = 1.3, intercept = FALSE) ## ---- list coefficients rownames(summary(mod.8)$coefficients) ## [1] &quot;(Intercept)&quot; &quot;slope&quot; &quot;elev&quot; &quot;p_avg_y&quot; &quot;p_min_spr&quot; ## ---- Odds ratios and 95% CI ORs&lt;- exp(cbind(OR = coef(mod.8), confint(mod.8)))[-1,] ## Intercept OR shouldn&#39;t be interpreted. ORs ## OR 2.5 % 97.5 % ## slope 1.8122110 1.31918128 2.5124546 ## elev 0.2050168 0.08805340 0.4087784 ## p_avg_y 0.1129699 0.03650038 0.3144093 ## p_min_spr 8.7945029 3.23875000 26.0283675 ## ----- Assess variance inflation (&gt;5 is not good) vif(mod.8) ## slope elev p_avg_y p_min_spr ## 1.164546 1.024544 8.822865 8.437776 sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] gridExtra_2.2.1 viridis_0.4.0 viridisLite_0.2.0 ggplot2_2.2.1 ## [5] coefplot2_0.1.3.2 coda_0.19-1 MASS_7.3-45 AICcmodavg_2.1-0 ## [9] car_2.1-4 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.10 nloptr_1.0.4 plyr_1.8.4 ## [4] tools_3.3.3 digest_0.6.12 lme4_1.1-12 ## [7] tibble_1.2 gtable_0.2.0 evaluate_0.10 ## [10] nlme_3.1-131 lattice_0.20-34 mgcv_1.8-17 ## [13] Matrix_1.2-8 yaml_2.1.14 parallel_3.3.3 ## [16] SparseM_1.76 stringr_1.2.0 knitr_1.15.17 ## [19] raster_2.5-8 MatrixModels_0.4-1 stats4_3.3.3 ## [22] rprojroot_1.2 grid_3.3.3 nnet_7.3-12 ## [25] reshape_0.8.6 survival_2.40-1 VGAM_1.0-3 ## [28] rmarkdown_1.4 bookdown_0.3.16 sp_1.2-4 ## [31] minqa_1.2.4 magrittr_1.5 scales_0.4.1 ## [34] backports_1.0.5 htmltools_0.3.5 splines_3.3.3 ## [37] assertthat_0.1 pbkrtest_0.4-7 unmarked_0.11-0 ## [40] colorspace_1.3-2 xtable_1.8-2 labeling_0.3 ## [43] quantreg_5.29 stringi_1.1.3 lazyeval_0.2.0 ## [46] munsell_0.4.3 "],
["predicting-site-location-with-simple-additive-raster-sensitivity-analysis-using-r.html", "Chapter 8 Predicting site location with simple additive raster sensitivity analysis using R", " Chapter 8 Predicting site location with simple additive raster sensitivity analysis using R Matthew Harris 8.0.1 Simple Arbitrary Additive Weight Sensitivity Model This code demonstrates the process for building a simple arbitrary weight archaeological sensitivity model, selecting a model threshold, and model validation. This type of model is by no means the ‘best’ model for archaeological predictive modeling, but it is very common, a useful approach to sensitivity stratification, as well as a good example of working with raster data and model validation. The primary use for this approach is: Stratify areas of a landscape based on the additive arbitrary sensitivity weights of one or more variables. Some advantages include: Conceptually simple; easy to explain based on expert opinion Computationally simple Not based on sample of known sites Sidesteps some bias in site sampling Some drawbacks include: Central Limit Theorem challenges statistical basis Difficult to optimize Not based on sample of known sites No method for assessing variable importance Does not include the variation evident in known site locations Primary R techniques used in this script: reading/writing raster files using the raster package in R (e.g. reclassify, mask, etc…) Interactive maps with mapview package Creating your own function plotting with ggplot2 8.0.2 Study Area This study area is a relatively large study area (897336 raster cells at 10.63425 meter resolution, for 101,477,305 square meters) including 120 pre-contact archaeological sites from somewhere in the United States. The coordinates of the data have been altered to protect the location of the sites. For the purpose of mapping, the coordinates of the data have been relocated to map over Fargo, North Dakota. In an ideal world, more site location data would be free… 8.0.3 Code The code below follows the sequence of defining functions and packages, loading raster data, establishing weights, classifying rasters based on weights, summing all weighted rasters, validating with known site locations, plotting results. 8.0.4 Define functions I typically put all my functions at the beginning of my code once I have them in working order. Here I define a function that takes the predicted sensitivity and presence/absence of known sites and returns a series of performance metrics to demonstrate how well the model classifies known sites. This function utilizes the ROCR package to define performance metrics across all thresholds. # predict == predicted value from model raster # response == site present (1) or site absent (0) at each predicted cell location balance_threshold &lt;- function(predict, response) { perf &lt;- ROCR::performance(ROCR::prediction(predict, response), &quot;sens&quot;, &quot;spec&quot;) auc &lt;- ROCR::performance(ROCR::prediction(predict, response), &quot;auc&quot;) auc &lt;- round(auc@y.values[[1]],3) df &lt;- data.frame(Weight = perf@alpha.values[[1]], Spec = perf@x.values[[1]], Sens = perf@y.values[[1]], Back_pcnt = 1 - perf@x.values[[1]], Xover = abs((perf@y.values[[1]] + (1-perf@x.values[[1]]))-1)) df$kg &lt;- 1-((1-df$Spec)/df$Sens) df$reach &lt;- 1-((1-df$Sens)/df$Spec) df$reach &lt;- ifelse(df$reach == 1, 0, df$reach) # removing reach == 1 df &lt;- data.frame(apply(df,2,function(x) round(x,3))) sens_spec &lt;- df[which.max(df$Sens + df$Spec), &quot;Weight&quot;] xover &lt;- df[which.min(df$Xover), &quot;Weight&quot;] kg &lt;- df[which.max(df$kg), &quot;Weight&quot;] reach &lt;- df[which.max(df[which(df$reach &lt; 1),&quot;reach&quot;]), &quot;Weight&quot;] # max where it is not == 1 list(df = df, sens_spec = sens_spec, xover = xover, auc = auc, kg = kg, reach = reach) } 8.0.5 Load packages I also typically put all of the necessary packages at the top of my code and typically note why I used that package in a comment. Otherwise, I will forget for some of the less common packages. Using R typically requires using a series of packages for any analysis. This is a strength of R, but requires one to learn how various packages work and to find the package for the task at hand. Don’t be worried about using a bunch of packages, but try to keep the updated! library(&quot;raster&quot;) # for raster manipulation library(&quot;rgdal&quot;) # for raster processing library(&quot;dplyr&quot;) # for data processing library(&quot;mapview&quot;) # for interactive map plots library(&quot;ggplot2&quot;) # for plotting results library(&quot;ROCR&quot;) # for model validation functions library(&quot;RColorBrewer&quot;) # for raster color scale library(&quot;knitr&quot;) # for printing nicer tables library(&quot;viridis&quot;) # for color scale in ggplot 8.0.6 Load raster files The raster package is the workhorse of dealing with raster data in R. It is a large an complex package, but has good documentation and is pretty easy to learn. The files used here were saved as .tif files from ArcGIS. A critical part of making this work is to make sure that you set your environments in ArcGIS or QGIS to align the grid cells for each raster, use the same projection, and consistently mask the raster so they have the same number of rows and columns. This can all be done in R, but to simplify this script, I did it in ArcGIS and exported to tif files. The raster stack is a raster data structure that combines rasters of the same dimension into a single file for ease of use. This includes running one operation that works on each raster in the stack; as we will see. Variables: slope.tif is the percent slope of the landscape derived from a 1/3rd arc-second DEM ed_h2.tif is the euclidean distance to National Hydrology Dataset (NHD) stream features ed_h4.tif is the euclidean distance to National Wetland Dataset (NWD) wetland polygons sites.tif is the location of 120 known archaeological site as rasterized polygons data_loc &lt;- &quot;clip_raster/&quot; slope &lt;- raster(paste0(data_loc, &quot;slope.tif&quot;)) ed_h2 &lt;- raster(paste0(data_loc, &quot;ed_h2.tif&quot;)) ed_h4 &lt;- raster(paste0(data_loc, &quot;ed_h4.tif&quot;)) sites &lt;- raster(paste0(data_loc, &quot;sites.tif&quot;)) raster_vars &lt;- stack(slope, ed_h2, ed_h4, sites) 8.0.7 Construct weights The core component of this model is the establishing of arbitrary weights assigned to classes of each raster variable. These weights can be based on educated guesses, empirical evidence, or to test a theory. Likewise, the manner in which the raster is classified into regions to weight is equally arbitrary or empirically based. What is not happening here is the use of known data to find the optimum set of weights and data splits to best separate site locations from non-sites. That is a model that discriminates based on known sites and weights from a metric such as information value or through statistical means of classification such as logistic regression, random forests, or any number of models. Here, the weights and splits are based on regional literature for how micro-social camps type sites are possibly distributed relative to these variables. Known site locations obviously influence this understanding, but they are not directly calculated to create the weights. The structure of these tables needs to be three columns that depict the values of form, to, and value. These are the raster values to classify from and to and the weight to assign to that class. ### Slope weighting Models### slp_from &lt;- c(0, 3, 5, 8, 15) slp_to &lt;- c(3, 5, 8, 15, 99999) slp_wght &lt;- c(50, 30, 15, 5, 0) slp_rcls&lt;- cbind(slp_from, slp_to, slp_wght) ### Dist to h20 weighting Models### h20_from &lt;- c(0, 100, 200, 400, 800) h20_to &lt;- c(100, 200, 400, 800, 9999) h20_wght &lt;- c(60, 25, 10, 4, 1) h20_rcls &lt;- cbind(h20_from, h20_to, h20_wght) ### Dist to wetland weighting Models### wtl_from &lt;- c(0, 100, 200, 400, 800) wtl_to &lt;- c(100,200, 400, 800, 9999) wtl_wght &lt;- c(35, 25, 20, 15, 5) wtl_rcls &lt;- cbind(wtl_from, wtl_to, wtl_wght) print(slp_rcls) ## slp_from slp_to slp_wght ## [1,] 0 3 50 ## [2,] 3 5 30 ## [3,] 5 8 15 ## [4,] 8 15 5 ## [5,] 15 99999 0 print(h20_rcls) ## h20_from h20_to h20_wght ## [1,] 0 100 60 ## [2,] 100 200 25 ## [3,] 200 400 10 ## [4,] 400 800 4 ## [5,] 800 9999 1 # an example of a more fully formatted table knitr::kable(wtl_rcls, digits = 0, col.names = c(&quot;From&quot;, &quot;To&quot;, &quot;Weight&quot;), caption = &quot;Sensitivity Weights for Distance (m) to Wetlands (NWD)&quot;) (#tab:construct_weights)Sensitivity Weights for Distance (m) to Wetlands (NWD) From To Weight 0 100 35 100 200 25 200 400 20 400 800 15 800 9999 5 8.0.8 Reclassify rasters The code to reclassify the rasters is very straight forward. For each of the three variable, we indicate that particular raster from the stack using the indexing of a list and the weight table. The reclassify function in the raster package does the work for us. raster_vars[[&quot;slope&quot;]] &lt;- reclassify(raster_vars[[&quot;slope&quot;]], slp_rcls) raster_vars[[&quot;ed_h2&quot;]] &lt;- reclassify(raster_vars[[&quot;ed_h2&quot;]], h20_rcls) raster_vars[[&quot;ed_h4&quot;]] &lt;- reclassify(raster_vars[[&quot;ed_h4&quot;]], wtl_rcls) 8.0.9 Summing rasters Given that the weighted rasters are within the raster stack, the sum() function will add together all of the layers that we indicate; in this case [1:3]. These are the weighted slope, ed_h2, and ed_h4 rasters. model_sum &lt;- sum(raster_vars[[1:3]]) base plot of model_sum ggplot of model_sum coords &lt;- coordinates(model_sum) x &lt;- data.frame(x = coords[,&quot;x&quot;], y = coords[,&quot;y&quot;], value = as.vector(model_sum)) ggplot(x, aes(x = x, y = y, fill = value)) + geom_raster(hjust = 0, vjust = 0) + theme_minimal() + viridis::scale_fill_viridis() + labs(title = &quot;Summed Sensitivity Weights&quot;, caption = &quot;Projection: Albers Conic Equal Area&quot;) 8.0.10 Clip sites In order to validate the weighting scheme and assess model performance, it is necessary to find out the summed weight value at known site locations. With this information, one can see if the summed weights are able to discriminate the parts of the landscape that are known to contain sites vs. the summed weights of the study area overall. Ideally, a model will give high summed weights to where sites are known and lower weights on average to where there are no sites; e.g. the background. However, since the purpose of this model is to identify areas of site potential/sensitivity, we need it to have some areas of high summed weights, but no known sites. The trick of any model type is to balance the size of this false-positive area (until survey proves otherwise) against the false-positive areas where we misclassify sites as absent. Further along we will use performance metrics to try to find the weight threshold that achieves this balance. The mask() function of the raster package is used to clip the summed weight raster model_sum by the known site locations. sites_sum &lt;- mask(model_sum, raster_vars[[&quot;sites&quot;]]) 8.0.10.1 Mapview Spatial data can be easily rendered in a ‘slippy-map’ format with the mapview package. This allows for overlays, base maps, legends, zooming, and panning with a very easy to use function. m &lt;- mapview(model_sum, col.regions = viridisLite::viridis, alpha = 0.75, maxpixels = 897336) + mapview(sites_sum, col.regions = viridisLite::magma, maxpixels = 897336) m 8.0.11 Model Validation “All models a wrong, but some are useful” ~ G. Box Box goes on to explain this in detail later by adding that a model’s usefulness is dependent on its purpose. Further, its purpose should not be figured out after the model is built. This understanding of purpose -&gt; mechanism -&gt; model -&gt; validation is important in framing the model building process. Model validation is a hugely important topic and deserve a thorough treatment on its own. However, we will cover a basic validation here by comparing the model’s ability to distinguish the location of known sites form the environmental background, as well as quantifying the balance of model errors vs. success. The former is important in understanding the model’s bias and the latter important in balancing the models variance. See this post for a great introduction. 8.0.11.1 Model discrimination If your study area has known sites, they can be used to test how well the model isolates the location of known sites and potentially the locations of yet-to-be-known sites. If known sites in your study area were used to derive the model weights, then this process should utilize a set of test sites that are held out from the model construction. Other techniques such as k-folds cross-validation, bootstrapping, and generalization error estimations are common approaches in approximating unbiased model performance. In this demo the model performance is based on the choice of model variables and the weights assigned to them. Based on the assumptions that we chose variables that are able to distinguish where settlement has occurred and that we assigned higher weights to areas of the landscape that are more likely to contain sites, our expectation is that when the score are summed, known sites will map on to higher weighted areas than the environmental background. To test this, we can extract the weights from the model_sum and sites_sum raster layers and plot the density of weights for each. # build a data.frame of weights and assign labels for sites and background sum_dat &lt;- data.frame(wght = c(model_sum@data@values, sites_sum@data@values), model = c(rep(&quot;background&quot;, length(model_sum)), rep(&quot;site&quot;, length(sites_sum)))) %&gt;% mutate(class = ifelse(model == &quot;site&quot;, 1, 0)) %&gt;% na.omit() # plot the wieghts as a density to compare ggplot(sum_dat, aes(x = wght, group = model, fill = model)) + geom_density(alpha = 0.55, adjust = 2) + scale_x_continuous(limits = c(0,200), expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) + labs(x=&quot;Summed Weight&quot;, y=&quot;Density&quot;, title=&quot;Density of Summed Weights by Class&quot;) + theme_bw() Or as a boxplot… # plot the wieghts as a boxplots to compare ggplot(sum_dat, aes(x = model, y = wght, group = model, fill = model)) + geom_boxplot() + labs(x=&quot;Classification&quot;, y=&quot;Summed Weight&quot;, title=&quot;Summed Weights by Class&quot;) + theme_bw() 8.0.11.2 Performance and thresholds In the section above, we performed simple model validation to visually assess the bias in of the model, or to put it another way, how well the model approximates the general sensitivity of settlement locations given variables, weights, and test site locations. Clearly there could be better weights, different variables, other test site samples, or no signal/mechanism to be found. The model’s success or failure to validate is not dependent on what could be, but what is based on the above vis a vis the model’s purpose. If the model’s bias is too great to achieve the model’s purpose given your assumptions, then you will need to reevaluate. If the model appears to be approaching your desired target, then now is a good time to assess its performance. In a classification model, performance depends on choosing thresholds. This is one of the most important yet glossed-over topics across many modeling applications. The summed weights of our model give a continuous distribution of values, however in a classification setting is it desirable to split these weights into present vs. absent, or high, medium, and low classes. An example of how threshold and performance are related and why it is so important is as follows. If a client asked me to make a model of sensitivity, I can guarantee them a perfect model that never fails. I do this by setting my present/absent threshold at model_sum == 0. I can guarantee that this model will identify the location of every unknown site only because it classifies the entire study are as site-likely. Sure, the client had to survey every square-meter of the study area, but every site will be found (assuming 100% identifiability). Of course, everyone knows this is not how the real world works and in this scenario, there is no reason to even make a model! Alternatively, if the client has a very slim budget, I can pick a threshold model_sum == max(model_sum) that classifies the entire study area as site-unlikely and I can guarantee that no site will be found. Of course, this is terrible from an archaeologist’s point of view, but it illustrates that the “best” threshold is somewhere in the middle. We can find that middle ground through various means, though the general approach is to balance the two extremes to optimize one or numerous metrics. Below we will evaluate for the metrics of Sensitivity, Specificity, Area Under the Curve (AUC), and the classic Kvamme Gain. Wikipedia has a good overview of Sensitvity, Specificity, and realted metrics. Below we use the performance() and prediction() functions in the ROCR package to derive the model’s Sensitivity (true positive rate) and Specificity (true negative rate) at each a threshold for each summed weight. So in this model, there are 54 unique combinations of weights, so it calculates 55 values for sensitivity and specificity. We also calculates a series of metrics that can be used to obtain a threshold value on which to classify site-likely vs. site-unlikely. They are: Kvamme Gain, Xover, Sens-Spec, and reach. # model_pred &lt;- prediction(sum_dat$wght, sum_dat$class) %&gt;% # performance(., &quot;sens&quot;, &quot;spec&quot;) model_pref &lt;- balance_threshold(sum_dat$wght, sum_dat$class) Kvamme Gain (KG) is a classic threshold metric in APM defined as 1 - (% background / % sites) which translates to 1 - ((1 - Specificity) / Sensitivity). The Reach metric is essentially the inverse of the KG, 1 - ((1 - Sensitvity) / Specificity), Sens-Spec is threshold that maximizes for both sensitivity and specificity, and Xover is the balance of sensitivity and specificity; this is the threshold illustrated by Kvamme (1998, pg 391, figure 8.11(B)). Each of these threshold give us a different justification for our model and which we chose should depend on any number of criteria including the model itself, use of the model, policy implications, funding, research, etc… for this demo, we will be using Xover as the balance of the true positive and true negative error rates. To visualize these thresholds and the trade-off they represent, we can plot how sensitivity and specificity change across each potential threshold value (weight) and overlay the four thresholds introduced above. To plot this, the output from the balance_threshold() function needs to be formatted into a long format using tidy::gather(). xover_dat &lt;- tidyr::gather(model_pref$df, metric, value, -Weight, -kg, -Back_pcnt, -Xover, -reach) threshold_dat &lt;- data.frame(threshold = c(&quot;kg&quot;, &quot;reach&quot;, &quot;X-over&quot;, &quot;Sens-Spec&quot;), weight = c(model_pref$kg, model_pref$reach, model_pref$xover, model_pref$sens_spec)) ggplot() + geom_line(data = xover_dat, aes(x = Weight, y = value, group = metric, color = metric), size=1) + geom_linerange(data = threshold_dat, aes(x = weight, ymin = 0, ymax = 1, linetype = threshold)) + scale_x_continuous(breaks=seq(0,200,5), labels = seq(0,200,5)) + scale_y_continuous(breaks=seq(0,1,0.1), labels = seq(0,1,0.1)) + labs(title = &quot;Sensitivity and Specificity at Optimized Metrics&quot;) + theme_bw() + theme( axis.text.x = element_text(angle = 90, hjust = 1) ) From this plot, we can see that: The KG threshold optimizes to maximize the amount of background relative to correct site predictions The reach threshold optimizes to maximize the percent correct sites over background The Sens-Spec threshold maximizes both sensitivity and specificity together, and The Xover threshold finds the point at which the sensitivity equals specificity Xover and Sens-Spec will typically be pretty close to each other and KG and reach will typically be biased towards background and sites respectively. In this demo we will assume that the maximization of sensitivity and specificity represented Sens-Spec achieves our goals and proceed with using the summed weight value of 55 to classify the model into site-likely and site-unlikely. This is achieved using the raster::reclassify() function as used earlier to weight the rasters. class_rcls &lt;- matrix(c(-Inf, model_pref$sens_spec, 0, model_pref$sens_spec, Inf, 1), ncol=3, byrow=TRUE) model_class &lt;- reclassify(model_sum, class_rcls, right = FALSE) 8.0.12 Results Based on the model and the chosen threshold, we can quantify the results in terms of the percent of site-present cells (not specifically % of total sites in this case) defined by sensitivity versus the percent of the model classified as site-likely defined as 1 - specificity (calculated as Back-pcnt in the table below). With this specific model, we are able to correctly classify 68.4 percent of the known site-present raster cells within 45.6 percent of the study area, for a KG of 0.334 model_pref[[&quot;df&quot;]] %&gt;% dplyr::filter(Weight == model_pref$sens_spec) ## Weight Spec Sens Back_pcnt Xover kg reach ## 1 55 0.544 0.684 0.456 0.139 0.334 0.419 8.0.12.1 Final Model The final classified raster and sites are rendered with mapview(). *unfortunately this is not rendering in the knit R markdown version m &lt;- mapview(model_class, col.regions = viridisLite::viridis, alpha = 0.75, maxpixels = 897336) + mapview(sites_sum, col.regions = viridisLite::magma, maxpixels = 897336) 8.0.12.2 sessionInfo() environment parameters sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] viridis_0.4.0 viridisLite_0.2.0 knitr_1.15.17 ## [4] RColorBrewer_1.1-2 ROCR_1.0-7 gplots_3.0.1 ## [7] ggplot2_2.2.1 mapview_1.2.0 leaflet_1.1.0 ## [10] dplyr_0.5.0 rgdal_1.2-5 raster_2.5-8 ## [13] sp_1.2-4 ## ## loaded via a namespace (and not attached): ## [1] gtools_3.5.0 lattice_0.20-34 colorspace_1.3-2 ## [4] htmltools_0.3.5 stats4_3.3.3 base64enc_0.1-3 ## [7] yaml_2.1.14 R.oo_1.21.0 DBI_0.6 ## [10] R.utils_2.5.0 foreach_1.4.3 plyr_1.8.4 ## [13] stringr_1.2.0 munsell_0.4.3 gtable_0.2.0 ## [16] R.methodsS3_1.7.1 htmlwidgets_0.8 caTools_1.17.1 ## [19] codetools_0.2-15 evaluate_0.10 labeling_0.3 ## [22] latticeExtra_0.6-28 httpuv_1.3.3 crosstalk_1.0.0 ## [25] gdalUtils_2.0.1.7 highr_0.6 Rcpp_0.12.10 ## [28] KernSmooth_2.23-15 xtable_1.8-2 backports_1.0.5 ## [31] scales_0.4.1 satellite_0.2.0 gdata_2.17.0 ## [34] jsonlite_1.3 webshot_0.4.0 mime_0.5 ## [37] gridExtra_2.2.1 png_0.1-7 digest_0.6.12 ## [40] stringi_1.1.3 bookdown_0.3.16 shiny_1.0.0 ## [43] grid_3.3.3 rprojroot_1.2 bitops_1.0-6 ## [46] tools_3.3.3 magrittr_1.5 lazyeval_0.2.0 ## [49] tibble_1.2 tidyr_0.6.1 assertthat_0.1 ## [52] rmarkdown_1.4 iterators_1.0.8 R6_2.2.0 "],
["landscape-based-hypothesis-testing-in-r.html", "Chapter 9 Landscape-based Hypothesis Testing in R 9.1 Introduction 9.2 Learning goals 9.3 Loading necessary packages 9.4 Defining the study area 9.5 Reading “site” locations from a table and cropping to a study area 9.6 Visualizing site locations 9.7 Downloading landscape data with FedData 9.8 Are sites situated based on elevation?", " Chapter 9 Landscape-based Hypothesis Testing in R Kyle Bocinsky 9.1 Introduction Many region-scale analyses in archaeology begin with a simple question: How do site locations relate to landscape attributes, such as elevation, soil type, or distance to water or other resources. Such a question is the foundation of basic geospatial exploratory data analysis, and answering it for a set of landscape attributes is the first step towards doing more interesting things, from interpreting settlement patterns in the past (???), to the construction of sophisticated predictive models of site location (???; ???; ???), to guiding settlement decision frameworks in agent-based simulations (e.g., ???; ???; ???). In this tutorial, we will learn how to use R to load, view, and explore site location data, and perform a very basic statistical settlement pattern analysis relating site location to elevation. Of course, archaeological site locations are often sensitive information, and it wouldn’t be prudent to provide them in tutorial like this. So instead of using cultural site locations, we’ll use a point dataset for which we can make a reasonable hypothesis concerning landscape attributes: cell tower locations from the US Federal Communications Commission. The cell tower data are somewhat difficult to work with, so I’ve distilled a snapshot of the database (accessed on February 14, 2017), and posted it online for easy download. We’ll go through the process of downloading them later in this tutorial. The hypothesis we’ll be testing is that cell towers are positioned in unusually high places on the landscape. This is similar to hypotheses we might make about archaeological site locations, perhaps having to do with defensibility (e.g., ???; ???; ???) or intervisibility and signaling (e.g., ???; ???). This tutorial is an R Markdown HTML document, meaning that all of the code to perform the calculations presented here was run when this web page was built—the paper was compiled. “Executable papers” such as this one are fantastic for presenting reproducible research in such a way that data, analysis, and interpretation are each given equal importance. Feel free to use this analysis as a template for your own work. All data and code for performing this analysis are available on Github at https://github.com/bocinsky/r_tutorials. 9.2 Learning goals In this short tutorial, you will learn how to: Download files from the internet in R Read ESRI shapefiles and other spatial objects into R using the sf and raster packages Promote tabular data to spatial objects Crop spatial objects by overlaying them atop one another Generate interactive web-maps using the leaflet package Download federated datasets (including elevation data) using the FedData package Extract data from a raster for specific points Calculate and graph Monte Carlo sub-sampled kernel density estimates for random locations Calculate Monte Carlo sub-sampled Mann-Whitney U test statistics (a non-parametric equivalent to the Student’s t-test) We will also gloss over several other fun topics in R data analysis, so download the source to learn more! 9.3 Loading necessary packages Almost all R scripts require that you load packages that enable the functions you use in your script. We load our requisite packages here using the install_cran() function from the devtools library. install_cran() is nice because it will checck whether we have the most recent version of the packages, and only install the ones that are necessary. We also use the walk() function from the purrr package to quickly traverse a vector, but that’s a topic for another tutorial. knitr::opts_chunk$set(echo = TRUE) packages &lt;- c(&quot;magrittr&quot;, # For piping &quot;foreach&quot;, &quot;purrr&quot;, &quot;tibble&quot;, &quot;dplyr&quot;, &quot;tidyr&quot;, &quot;broom&quot;, # For tidy data analysis &quot;ggplot2&quot;,&quot;plotly&quot;, # For fancy graphs &quot;sf&quot;, &quot;sp&quot;, &quot;raster&quot;, # For spatial analysis &quot;leaflet&quot;, &quot;htmltools&quot;, # For fancy maps &quot;FedData&quot;) # install.packages(&quot;devtools&quot;) # devtools::install_cran(packages, repos = &quot;https://cran.rstudio.com/&quot;) # For downloading federated geospatial data purrr::walk(packages, library, character.only = TRUE) # Load all packages with no output # Create an output folder in our current working directory dir.create(&quot;OUTPUT&quot;, showWarnings = FALSE, recursive = FALSE) 9.4 Defining the study area All landscape-scale analyses start with the definition of a study area. Since the cell tower dataset with which we’ll be working covers the whole USA, we could really set our study area to be anywhere in the US. Here, we will download an 1:500,500 scale ESRI shapefile of counties in the United States available from the US Census, and pick a county to serve as our study area. I’m going to pick Whitman county, Washington, because that’s where I live; feel free to choose your own county! Files may be downloaded in R using many different functions, but perhaps the most straightforward is the download.file() function, which requires that you specify a url to the file you wish to download, and a destfile where the downloaded file should end up. As the counties shapefile is in a zip archive, we will also use the unzip() function, which requires you define an extraction directory (exdir). # Download the 1:500000 scale counties shapefile from the US Census if(!file.exists(&quot;OUTPUT/cb_2015_us_county_500k.zip&quot;)) download.file(&quot;http://www2.census.gov/geo/tiger/GENZ2015/shp/cb_2015_us_county_500k.zip&quot;, destfile = &quot;OUTPUT/cb_2015_us_county_500k.zip&quot;) # Unzip the file unzip(&quot;OUTPUT/cb_2015_us_county_500k.zip&quot;, exdir = &quot;OUTPUT/counties&quot;) Navigate to the exdir you specified and check to make sure the shapefile is there. Now it’s time to load the shapefile into R. We’ll be using the st_read() function from the sf library, which reads a shapefile (and many other file formats) and stores it in memory as a “simple feature” object of the sf library. To read a shapefile, the st_read() function requires only the path to the shapefile with the “.shp” file extension. Other spatial file formats have different requirements for st_read(), so read the documentation (help(st_read)) for more information. # Load the shapefile census_counties &lt;- sf::st_read(&quot;OUTPUT/counties/cb_2015_us_county_500k.shp&quot;) ## Reading layer `cb_2015_us_county_500k&#39; from data source `E:\\My Documents\\My Papers\\conferences\\SAA2017\\How-To-Do-Archaeological-Science-Using-R\\08_Bocinsky\\OUTPUT\\counties\\cb_2015_us_county_500k.shp&#39; using driver `ESRI Shapefile&#39; ## converted into: POLYGON ## Simple feature collection with 3233 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.1489 ymin: -14.5487 xmax: 179.7785 ymax: 71.36516 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs # Inspect the spatial object head(census_counties) ## Simple feature collection with 6 features and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -88.47323 ymin: 31.18113 xmax: -85.04931 ymax: 33.00682 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs ## STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND ## 1 01 005 00161528 0500000US01005 01005 Barbour 06 2291820706 ## 2 01 023 00161537 0500000US01023 01023 Choctaw 06 2365954971 ## 3 01 035 00161543 0500000US01035 01035 Conecuh 06 2201896058 ## 4 01 051 00161551 0500000US01051 01051 Elmore 06 1601876535 ## 5 01 065 00161558 0500000US01065 01065 Hale 06 1667804583 ## 6 01 109 00161581 0500000US01109 01109 Pike 06 1740741211 ## AWATER geometry ## 1 50864677 MULTIPOLYGON(((-85.748032 3... ## 2 19059247 MULTIPOLYGON(((-88.473227 3... ## 3 6643480 MULTIPOLYGON(((-87.427204 3... ## 4 99850740 MULTIPOLYGON(((-86.413335 3... ## 5 32525874 MULTIPOLYGON(((-87.870464 3... ## 6 2336975 MULTIPOLYGON(((-86.199408 3... When we inspect the census_counties object, we see that it is a simple feature collection with 3233 features. Simple feature collection objects are simply data frames with an extra column for spatial objects, and metadata including projection information. Now it’s time to extract just the county we want to define our study area. Because a Simple feature collection object extends the data.frame class, we can perform selection just as we would with a data.frame. We do that here using the filter() function from the dplyr package: # Select Whitman county my_county &lt;- dplyr::filter(census_counties, NAME == &quot;Whitman&quot;) # Inspect the spatial object my_county ## Simple feature collection with 1 feature and 9 fields ## geometry type: MULTIPOLYGON ## dimension: XY ## bbox: xmin: -179.1489 ymin: -14.5487 xmax: 179.7785 ymax: 71.36516 ## epsg (SRID): 4269 ## proj4string: +proj=longlat +datum=NAD83 +no_defs ## STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND ## 1 53 075 01533501 0500000US53075 53075 Whitman 06 5591999422 ## AWATER geometry ## 1 47939386 MULTIPOLYGON(((-118.249122 ... As you can see, the spatial object now has only one feature, and it is Whitman county! We’ll map it in a minute, but first let’s do two more things to make our lives easier down the road. We’ll be mapping using the leaflet package, which makes pretty, interactive web maps. For leaflet, we need the spatial data to be in geographic coordinates (longitude/latitude) using the WGS84 ellipsoid. Here, we’ll transform our county to that projection system using the st_transform() function. This code chunk also uses something new: the pipe operator %&gt;% from the magrittr package. The pipe operator enables you to “pipe” a value forward into an expression or function call—whatever is on the left hand side of the pipe becomes the first argument to the function on the right hand side. So, for example, to find the mean of the numeric vector c(1,2,3,5) by typing mean(c(1,2,3,5)), we could instead use the pipe: c(1,2,3,5) %&gt;% mean(). Try running both versions; you should get 2.75 for each. The pipe isn’t much use for such a simple example, but becomes really helpful for code readability when chaining together many different functions. The compound assignment operator %&lt;&gt;% pipes an object forward into a function or call expression and then updates the left hand side object with the resulting value, and is equivalent to x &lt;- x %&gt;% fun() # Transform to geographic coordinates my_county %&lt;&gt;% sf::st_transform(&quot;+proj=longlat +datum=WGS84&quot;) 9.5 Reading “site” locations from a table and cropping to a study area Alright, now that we’ve got our study area defined, we can load our “site” data (the cell towers). We can use the read_csv() function from the readr library to read the cell tower locations from a local file, which we download using download.file(). We’ll read them and then print them. # Download cell tower location data if(!file.exists(&quot;OUTPUT/cell_towers.csv&quot;)) download.file(&quot;https://raw.githubusercontent.com/bocinsky/r_tutorials/master/data/cell_towers.csv&quot;, destfile = &quot;OUTPUT/cell_towers.csv&quot;) # Load the data cell_towers &lt;- readr::read_csv(&quot;OUTPUT/cell_towers.csv&quot;) cell_towers ## # A tibble: 101,388 × 5 ## `Unique System Identifier` `Entity Name` ## &lt;int&gt; &lt;chr&gt; ## 1 96974 Crown Castle GT Company LLC ## 2 96978 SBC TOWER HOLDINGS LLC ## 3 96979 CHAMPAIGN TOWER HOLDINGS LLC ## 4 96981 CCATT LLC ## 5 96989 Crown Atlantic Company LLC ## 6 96999 SBC TOWER HOLDINGS LLC ## 7 97007 American Towers, LLC ## 8 97011 UNITED STATES CELLULAR CORPORATION ## 9 97014 Crown Castle South LLC ## 10 97022 Crown Castle GT Company LLC ## # ... with 101,378 more rows, and 3 more variables: `Height of ## # Structure` &lt;dbl&gt;, Latitude &lt;dbl&gt;, Longitude &lt;dbl&gt; As you can see, the cell tower data includes basic identification information as well as geographic coordinates in longitude and latitude. We can use the coordinate data to promote the data frame to a spatial object using the st_as_sf() function. Finally, we can write a simple function to select only the cell towers in our study area using the filter() function again, along with the st_within() function from the sf package. # Promote to a simple feature collection cell_towers %&lt;&gt;% sf::st_as_sf(coords = c(&quot;Longitude&quot;,&quot;Latitude&quot;), crs = &quot;+proj=longlat +datum=WGS84&quot;) # Select cell towers in our study area cell_towers %&lt;&gt;% dplyr::filter({ sf::st_within(., my_county, sparse = F) %&gt;% # This function returns a matrix as.vector() # coerce to a vector }) cell_towers ## Simple feature collection with 41 features and 3 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -176.6418 ymin: 13.27747 xmax: -64.68769 ymax: 71.31036 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 41 × 4 ## `Unique System Identifier` `Entity Name` ## &lt;int&gt; &lt;chr&gt; ## 1 597094 Qwest Corporation ## 2 597409 NORTHWEST LEASE ONE, L.L.C. ## 3 597902 SINCLAIR LEWISTON LICENSEE, LLC ## 4 598225 Inland Cellular LLC ## 5 598624 AVISTA CORPORATION ## 6 598630 AVISTA CORPORATION ## 7 599225 WASHINGTON STATE UNIVERSITY ## 8 609045 UNION PACIFIC RAILROAD ## 9 612883 PALOUSE COUNTRY INC. ## 10 2606107 Washington State University ## # ... with 31 more rows, and 2 more variables: `Height of ## # Structure` &lt;dbl&gt;, geometry &lt;simple_feature&gt; Now we see that there are 41 cell towers in our study area. 9.6 Visualizing site locations There are many different ways to visualize spatial data in R, but perhaps the most useful is using the leaflet package, which allows you to make interactive HTML maps that will impress your friends, are intuitive to a 4-year-old, and will thoroughly confuse your advisor. I’m not going to go through all of the syntax here, but in general leaflet allows you to layer spatial objects over open-source map tiles to create pretty, interactive maps. Here, we’ll initialize a map, add several external base layer tiles, and then overlay our county extent and cell tower objects. leaflet is provided by the good folks at RStudio, and is well documented here. Zoom in on the satellite view, and you can see the cell towers! # Create a quick plot of the locations leaflet(width = &quot;100%&quot;) %&gt;% # This line initializes the leaflet map, and sets the width of the map at 100% of the window addProviderTiles(&quot;OpenTopoMap&quot;, group = &quot;Topo&quot;) %&gt;% # This line adds the topographic map from Garmin addProviderTiles(&quot;OpenStreetMap.BlackAndWhite&quot;, group = &quot;OpenStreetMap&quot;) %&gt;% # This line adds the OpenStreetMap tiles addProviderTiles(&quot;Esri.WorldImagery&quot;, group = &quot;Satellite&quot;) %&gt;% # This line adds orthoimagery from ESRI addProviderTiles(&quot;Stamen.TonerLines&quot;, # This line and the next adds roads and labels to the orthoimagery layer group = &quot;Satellite&quot;) %&gt;% addProviderTiles(&quot;Stamen.TonerLabels&quot;, group = &quot;Satellite&quot;) %&gt;% addPolygons(data = my_county, # This line adds the Whitman county polygon label = &quot;My County&quot;, fill = FALSE, color = &quot;red&quot;) %&gt;% addMarkers(data = cell_towers, popup = ~htmlEscape(`Entity Name`)) %&gt;% # This line adds cell tower locations addLayersControl( # This line adds a controller for the background layers baseGroups = c(&quot;Topo&quot;, &quot;OpenStreetMap&quot;, &quot;Satellite&quot;), options = layersControlOptions(collapsed = FALSE), position = &quot;topleft&quot;) It’s obvious from this map that the cell towers aren’t merely situated to be in high places—they also tend to cluster along roads and in densely populated areas. 9.7 Downloading landscape data with FedData FedData is an R package that is designed to take much of the pain out of downloading and preparing data from federated geospatial databases. For an area of interest (AOI) that you specify, each function in FedData will download the requisite raw data, crop the data to your AOI, and mosaic the data, including merging any tabular data. Currently, FedData has functions to download and prepare these datasets: The National Elevation Dataset (NED) digital elevation models (1 and 1/3 arc-second; USGS) The National Hydrography Dataset (NHD) (USGS) The Soil Survey Geographic (SSURGO) database from the National Cooperative Soil Survey (NCSS), which is led by the Natural Resources Conservation Service (NRCS) under the USDA, The Global Historical Climatology Network (GHCN) daily weather data, coordinated by the National Oceanic and Atmospheric Administration (NOAA), The Daymet gridded estimates of daily weather parameters for North America, version 3, available from the Oak Ridge National Laboratory’s Distributed Active Archive Center (DAAC), and The International Tree Ring Data Bank (ITRDB), coordinated by NOAA. In this analysis, we’ll be downloading the 1 arc-second elevation data from the NED under our study area. The FedData functions each require four basic parameters: A template defining your AOI, supplied as a spatial*, raster*, or spatial extent object A character string (label) identifying your AOI, used for file names A character string (raw.dir) defining where you want the raw data to be stored; this will be created if necessary A character string (extraction.dir) defining where you want the extracted data for your AOI to be stored; this will also be created if necessary Here, we’ll download the 1 arc-second NED with the get_ned() function from FedData, using the my_county object that we created above as out template, and local relative paths for our raw.dir and extraction.dir. We’ll download and prepare the NED, and then plot it using the basic plot() function. # Download the 1 arc-second NED elevation model for our study area if(!dir.exists(&quot;OUTPUT/RAW/NED&quot;)){ my_county_NED &lt;- FedData::get_ned(template = my_county %&gt;% as(&quot;Spatial&quot;), # FedData functions currently expect Spatial* objects, so coerce to a Spatial* object label = &quot;my_county&quot;, raw.dir = &quot;OUTPUT/RAW/NED&quot;, extraction.dir = &quot;OUTPUT/EXTRACTIONS/NED&quot;) # Mask to the county (rather time consuming) my_county_NED[is.na(over(my_county_NED %&gt;% as(&quot;SpatialPixels&quot;), my_county %$% geometry %&gt;% sf::st_transform(projection(my_county_NED)) %&gt;% as(&quot;Spatial&quot;)))] &lt;- NA # save this object so we can use it again saveRDS(my_county_NED, &quot;OUTPUT/my_county_NED.rds&quot;) } else { my_county_NED &lt;- readRDS(&quot;OUTPUT/my_county_NED.rds&quot;) } # Print the my_county_NED object my_county_NED ## class : RasterLayer ## dimensions : 3037, 4357, 13232209 (nrow, ncol, ncell) ## resolution : 0.0002777778, 0.0002777778 (x, y) ## extent : -118.2494, -117.0392, 46.41694, 47.26056 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=longlat +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +no_defs ## data source : in memory ## names : my_county_NED_1 ## values : 163.8032, 1221.359 (min, max) # Plot the my_county_NED object my_county_NED %&gt;% plot() # Plot the my_county polygon over the elevation raster my_county %$% geometry %&gt;% plot(add = T) The NED elevation data was downloaded for our study area, cropped to the rectangular extent of the county, and then masked to the county. 9.8 Are sites situated based on elevation? Alright, now that all of our data are in place, we can get to the meat of our analysis: are cell towers in especially high places in Whitman county? Here, we treat Whitman county as the decision space within which some past cellular engineers decided to build towers—our null hypothesis is that there is no relationship between elevation and cell tower location. Put another way, our task is to rule out whether cell towers were likely to have been placed randomly across the landscape. We’ll first extract the elevations for the cell towers, then calculate a probability density estimate for the cell towers, estimate a probability density curve for the landscape using Monte Carlo resampling, and finally compare the two distributions graphically and numerically using a Mann-Whitney U test. 9.8.1 Extract site elevations Extracting the cell tower elevations is straightforward using the extract() function from the raster package. # Extract cell tower elevations from the study area NED values # knit in bookdown gives error with sf -&gt; spatial, so here&#39;s a workaround library(rgdal) library(sp) cell_towers_sp &lt;- SpatialPoints(st_coordinates(cell_towers), CRS(proj4string(my_county_NED))) cell_towers_spdf &lt;- SpatialPointsDataFrame(cell_towers_sp, data.frame(cell_towers)) cell_towers$`Elevation (m)` &lt;- raster::extract(my_county_NED, cell_towers_spdf) # cell_towers$`Elevation (m)` &lt;- raster::extract(my_county_NED, # as(cell_towers, &quot;Spatial&quot;)) cell_towers ## Simple feature collection with 41 features and 4 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: -176.6418 ymin: 13.27747 xmax: -64.68769 ymax: 71.31036 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## # A tibble: 41 × 5 ## `Unique System Identifier` `Entity Name` ## &lt;int&gt; &lt;chr&gt; ## 1 597094 Qwest Corporation ## 2 597409 NORTHWEST LEASE ONE, L.L.C. ## 3 597902 SINCLAIR LEWISTON LICENSEE, LLC ## 4 598225 Inland Cellular LLC ## 5 598624 AVISTA CORPORATION ## 6 598630 AVISTA CORPORATION ## 7 599225 WASHINGTON STATE UNIVERSITY ## 8 609045 UNION PACIFIC RAILROAD ## 9 612883 PALOUSE COUNTRY INC. ## 10 2606107 Washington State University ## # ... with 31 more rows, and 3 more variables: `Height of ## # Structure` &lt;dbl&gt;, `Elevation (m)` &lt;dbl&gt;, geometry &lt;simple_feature&gt; 9.8.2 Calculate kernel density curves for sites and landscape We can calculate kernel density curves using the density() function available in all R installations. This code block gets a little complicated. The first section is strightforward: we estimate the probability density for all elevations between 150 and 1250 masl (the domain of the county elevation). The second section is a bit more complicated: we estimate probability densities for 99 random samples from the elevation data. (You would probably want to draw more resamples than this in a real analysis). Each sample is of the same number of sites as there are cell towers. This is called Monte Carlo resampling. The code section performs the sampling, then calculates a 95% confidence interval for the sampled data using quantiles. We will use the foreach package (and its %do% function) to repeat and output the resampling. # Calculate the cell tower densities # ------------------------- cell_towers_densities &lt;- cell_towers %$% `Elevation (m)` %&gt;% density(from = 150, to = 1250, n = 1101) %&gt;% tidy() %&gt;% tibble::as_tibble() %&gt;% dplyr::mutate(y = y * 1101) %&gt;% dplyr::rename(Elevation = x, Frequency = y) # Calculate possible densities across the study area using resampling # ------------------------- # Load the NED elevations into memory for fast resampling my_county_NED_values &lt;- my_county_NED %&gt;% values() %&gt;% na.omit() # Drop all masked (NA) locations # Draw 99 random samples, and calculate their densities my_county_NED_densities &lt;- foreach::foreach(n = 1:99, .combine = rbind) %do% { my_county_NED_values %&gt;% sample(nrow(cell_towers), replace = FALSE) %&gt;% density(from = 150, to = 1250, n = 1101) %&gt;% broom::tidy() %&gt;% tibble::as_tibble() %&gt;% dplyr::mutate(y = y * 1101) } %&gt;% dplyr::group_by(x) %&gt;% purrr::by_slice(function(x){ quantile(x$y, probs = c(0.025, 0.5, 0.975)) %&gt;% t() %&gt;% broom::tidy() }, .collate = &quot;rows&quot;) %&gt;% magrittr::set_names(c(&quot;Elevation&quot;, &quot;Lower CI&quot;, &quot;Frequency&quot;, &quot;Upper CI&quot;)) 9.8.3 Plot the kernel density curves We’ll perform a statistical test on the cell tower and resampled elevation data in a minute, but first it is just as helpful to view a graph of the two data sets. Like all things, R has many different ways of graphing data, but the ggplot2 package from Hadley Wickam is fast becoming the framework-du jour for graphics in R. The plotly package can be used to effortly convert ggplot2 graphs into interactive HTML graphics. ggplot2 uses a pipe-like system for building graphs, where graphical elements are appended to one-another using the + operator. Hover over the plot to explore it interactively. # Plot both distributions using ggplot2, then create an interactive html widget using plotly. g &lt;- ggplot() + geom_line(data = my_county_NED_densities, mapping = aes(x = Elevation, y = Frequency)) + geom_ribbon(data = my_county_NED_densities, mapping = aes(x = Elevation, ymin = `Lower CI`, ymax = `Upper CI`), alpha = 0.3) + geom_line(data = cell_towers_densities, mapping = aes(x = Elevation, y = Frequency), color = &quot;red&quot;) ggplotly(g)# Create the HTML widget This plot is revealing. The landscape data (represented by the black line and gray confidence interval) is left skewed and has two modes at c. 550 and 750 masl. In contrast, the cell tower data has a single dominant mode at c. 780 masl, and is slightly right skewed. From this visual investigation alone, we can see that the cell tower locations were unlikely to have been randomly sampled from the landscape as a whole. 9.8.4 Mann-Whitney U test comparing non-normal distributions Because neither of these distributions are statistically normal, we will use the nonparametric Mann-Whitney U test (also known as a Wilcoxon test) to test whether the cell tower locations were likely a random sample of locations drawn from our study area. Again, we’ll use Monte Carlo resampling to generate confidence intervals for our test statistic. Finally, we will output the test data to a comma-seperated values (CSV) file for inclusion in external reports. # Draw 999 random samples from the NED, and compute two-sample Wilcoxon tests (Mann-Whitney U tests) my_county_Cell_MWU &lt;- foreach(n = 1:99, .combine = rbind) %do% { my_county_sample &lt;- my_county_NED_values %&gt;% sample(nrow(cell_towers), replace = FALSE) %&gt;% wilcox.test(x = cell_towers$`Elevation (m)`, y = ., alternative = &quot;greater&quot;, exact = FALSE) %&gt;% tidy() %&gt;% tibble::as_tibble() } %&gt;% dplyr::select(statistic, p.value) # Get the median test statistic and 95% confidence interval my_county_Cell_MWU &lt;- foreach::foreach(prob = c(0.025,0.5,0.975), .combine = rbind) %do% { my_county_Cell_MWU %&gt;% dplyr::summarise_all(quantile, probs = prob) } %&gt;% t() %&gt;% magrittr::set_colnames(c(&quot;Lower CI&quot;,&quot;Median&quot;,&quot;Upper CI&quot;)) %&gt;% magrittr::set_rownames(c(&quot;U statistic&quot;,&quot;p-value&quot;)) # Write output table as a CSV my_county_Cell_MWU %T&gt;% write.csv(&quot;OUTPUT/Mann_Whitney_results.csv&quot;) ## Lower CI Median Upper CI ## U statistic 1.195150e+03 1.324000e+03 1.460050e+03 ## p-value 4.914482e-09 3.743561e-06 5.141019e-04 The results of the Mann-Whitney U two-sample tests show that it is highly unlikely that the cell towers in Whitman county were randomly placed across the landscape (median U statistic = 1324, median p-value = 3.743560710^{-6}). We can infer from the graphical analysis above that the cell towers were placed on unusually high places across the landscape. 9.8.4.1 Conclusions R is extremely powerful as a geo-analytic tool, and encountering sophisticated code for the first time can be daunting. But R can also be useful for basic exploratory spatial analysis, quick-and-dirty statistical testing, and interactive data presentation. I hope this short tutorial gave you a taste of how useful R can be. 9.8.5 References cited sessionInfo() ## R version 3.3.3 (2017-03-06) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 7 x64 (build 7601) Service Pack 1 ## ## locale: ## [1] LC_COLLATE=English_Australia.1252 LC_CTYPE=English_Australia.1252 ## [3] LC_MONETARY=English_Australia.1252 LC_NUMERIC=C ## [5] LC_TIME=English_Australia.1252 ## ## attached base packages: ## [1] methods stats graphics grDevices utils datasets base ## ## other attached packages: ## [1] rgdal_1.2-5 FedData_2.4.5 htmltools_0.3.5 leaflet_1.1.0 ## [5] raster_2.5-8 sp_1.2-4 sf_0.4-1 plotly_4.5.6 ## [9] ggplot2_2.2.1 broom_0.4.2 tidyr_0.6.1 dplyr_0.5.0 ## [13] tibble_1.2 purrr_0.2.2 foreach_1.4.3 magrittr_1.5 ## ## loaded via a namespace (and not attached): ## [1] reshape2_1.4.2 lattice_0.20-34 colorspace_1.3-2 ## [4] viridisLite_0.2.0 yaml_2.1.14 base64enc_0.1-3 ## [7] foreign_0.8-67 DBI_0.6 plyr_1.8.4 ## [10] stringr_1.2.0 rgeos_0.3-22 munsell_0.4.3 ## [13] gtable_0.2.0 htmlwidgets_0.8 codetools_0.2-15 ## [16] psych_1.7.3.21 evaluate_0.10 labeling_0.3 ## [19] knitr_1.15.17 doParallel_1.0.10 httpuv_1.3.3 ## [22] crosstalk_1.0.0 parallel_3.3.3 Rcpp_0.12.10 ## [25] readr_1.1.0 xtable_1.8-2 udunits2_0.13 ## [28] scales_0.4.1 backports_1.0.5 jsonlite_1.3 ## [31] mime_0.5 mnormt_1.5-5 hms_0.3 ## [34] digest_0.6.12 stringi_1.1.3 bookdown_0.3.16 ## [37] shiny_1.0.0 ncdf4_1.15 grid_3.3.3 ## [40] rprojroot_1.2 tools_3.3.3 lazyeval_0.2.0 ## [43] data.table_1.10.4 lubridate_1.6.0 assertthat_0.1 ## [46] rmarkdown_1.4 httr_1.2.1 iterators_1.0.8 ## [49] R6_2.2.0 compiler_3.3.3 units_0.4-3 ## [52] nlme_3.1-131 "],
["open.html", "Chapter 10 Open Review 10.1 FAQ about open review 10.2 Privacy and Consent Policy", " Chapter 10 Open Review Open Review means that you can freely read the book and easily help to make it better. You can offer suggestions by make annotations using hypothes.is, an open source annotation system. This is a very simple system for interacting with the book. If you are familiar with GitHub or Git, you can also comment using GitHub’s issue tracker for the book, or make a pull request. In addition to these feedback options, this website for the book will be collecting your implicit feedback by tracking the readership and abandonment rate of each section of the book. Open Review takes place before and at the same time as the book publisher’s peer review. The feedback from Open Review and peer review will be used to create a revised manuscript. The Open Review period will end when the final manuscript is submitted to the publisher. The concept of Open Review, as it is implemented here, is taken from Matthew Salganik’s Open Review Toolkit. Much of the text on this page comes from the Open Review Toolkit About page and the Open Review Toolkit Privacy and Consent page 10.1 FAQ about open review 10.1.1 What kind of feedback are you looking for? Open Review is not just about catching typos. Rather, Open Review is designed to collect all types of feedback, and I’d particularly welcome any feedback that you have about the substance of the book. Are there sections that you find particularly confusing? Are their points that you find particularly important? Am I making claims that you think need to be refined? Are there parts of the book that you think should be removed? When in doubt, I think you should follow one of the main principles at Wikipedia: Be bold. 10.1.2 Can I see the annotations that others are making? Yes, all the annotations are public. You can see them on right hand side of the each page or you can read them in stream form. 10.1.3 What are the benefits for readers? You get to read the manuscript and help make it better. 10.1.4 What are the benefits for authors and publishers? The Open Review process will benefit both authors and publishers, even if they have no interest in increasing access to knowledge. The process will lead to higher manuscript quality through the explicit and implicit feedback. Further, the Open Review process will provide valuable data that can be used during that marketing of the book. 10.1.5 Has anyone ever done something like this before? This is based on Matthew Salganik’s Open Review Toolkit. He’s written about some related efforts here: http://www.bitbybitbook.com/en/open-review/ 10.1.6 What kind of information are you collecting and how will that information be used? Please read our privacy and consent policy, below. 10.1.7 How I can learn more about traditional peer review of academic books? The AAUP recently published a report on best practices for peer review. 10.1.8 Can I do this with my book? Sure. Check out the code for this website at https://github.com/benmarwick/bookdown-ort/ for more about how we did it. 10.1.9 I have a different question about Open Review. How can I get in touch? Send an email to bmarwick@uw.edu 10.2 Privacy and Consent Policy 10.2.1 Overview On this website, we are making the complete text of the book available at no cost. While you are reading the book, we are measuring reader behavior in aggregate. For example, we are measuring which sections of the book get read most often. This data will help us improve the book. Everything we are doing is common on modern websites. We describe it in more detail below. 10.2.2 What information do we collect? We use Google Analytics to collect information about how you interact with this website. Further, like most websites, we use cookies to enhance your experience, gather general visitor information, and track visits to our website. Please refer to the “do we use cookies?” section below for information about cookies and how we use them. 10.2.3 How do we use your information? Any of the information that we collect may be used for research, to improve the book, and to help sell the book. 10.2.4 How do we protect your information? We implement a variety of security measures to maintain the safety of the information that you provide us. Most of the browsing information that we have is stored Google Analytics, and you can read more about their security and privacy principles. Annotations that you add are managed by hypothes.is, and you can read more about their terms of service. Our website is hosted by Github Pages, and you can read more about Github’s terms of service. 10.2.5 Do we use cookies? Yes. Cookies are small files that a site or its service provider transfers to your computer’s hard drive through your web browser (if you allow) that enables the sites or service providers systems to recognize your browser and capture and remember certain information. In order to offer you a better site experience, we use cookies to understand and save your preferences for future visits and to compile aggregate data about site traffic. 10.2.6 Do we disclose any information to outside parties? We do not sell, trade, or otherwise transfer to outside parties your personally identifiable information except trusted third parties who assist us in operating our website, conducting research, or providing a service to you, so long as those parties agree to keep this information confidential. We may also release your information when we believe release is appropriate to comply with the law, enforce our site policies, or protect ours or others’ rights, property, or safety. 10.2.7 Third party links Occasionally, at our discretion, we may include links to third party websites. These third party sites have separate and independent privacy policies. We, therefore, have no responsibility or liability for the content and activities of these linked sites. Nonetheless, we seek to protect the integrity of our site and welcome any feedback about these sites. 10.2.8 Website hosting Our website is hosted by Github Pages, and you can read more about Github’s terms of service. 10.2.9 Your consent By using our site, you consent to our privacy policy. 10.2.10 Questions If you have any questions, please send us an email bmarwick@uw.edu 10.2.11 Changes to our Privacy and Consent Policy We reserve the right to change our privacy policy from time to time at our sole discretion. Please periodically check this section to review the current version of the Privacy and Consent Policy. All our previous policies are presented below, and we will continue to update this page if any changes are needed. "]
]
